---
title: "Mass Univariate Analysis for Pupillometric Data: Cluster Permutation Test"
layout: single-sidebar
date: '2020-07-10'
slug: gp-pup
categories:
  - R
tags:
  - R
  - eye-tracking
  - pupil

subtitle: 'How to analyze pupil data'
summary: 'Show how to analyze pupil data using cluster based permutation testing'
authors: ["Jason Geller"]
lastmod: ''
featured: yes
image:
  caption: 
  placement: 2
  focal_point: 'Center'
  preview_only: no
output: html_document
bibliography: bib1.bib 
projects: []

---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>An article by <span class="citation"><a href="#ref-Reilly2019" role="doc-biblioref">Reilly et al.</a> (<a href="#ref-Reilly2019" role="doc-biblioref">2019</a>)</span> raised some important questions/issues for the field of pupillometry. One issue that has been bothering me as of late (hence the post) pertains to objectively defining a time window to look at. Although some commonalities exist across the literature, there is no uniform standard for isolating a peak range or delineating the width of this window. In this blog post I present a method we can use to address this issue. To do so, I will be using the example data set from the gazeR package (<span class="citation"><a href="#ref-Geller2" role="doc-biblioref">Geller et al.</a> (<a href="#ref-Geller2" role="doc-biblioref">n.d.</a>)</span>). This data comes from a lexical decison experiment where participants judged the lexicality of cursive and type-print stimuli.</p>
<div id="common-approaches" class="section level1">
<h1>Common Approaches</h1>
<p>In the pupillometry literature, I have seen several different analytic approaches to the above issue. Some will preform seperate statistical tests at each time point (via <em>t</em> tests, LMMs, ANOVAs, regressions, what have you), or they just ignore time altogether and aggregate the data and perfrom analyses on the mean and max. I do not like these approaches for a few reasons:</p>
<ul>
<li><p>There is a trade-off between power and temporal resolution (smaller time bins) and introducesbias in selection of time bins/windows. There is also the issues of multiple comparisons and autocorrelation</p></li>
<li><p>Does not take into account individual differences</p></li>
</ul>
<p>I wanted to highlight a method I believe is a good alternative: cluster-based permutation tests (henceforth CPT)</p>
</div>
<div id="cpt" class="section level1">
<h1>CPT</h1>
<p>Let’s first look at a graph of the data.</p>
<pre><code>## Error: Can&#39;t subset columns that don&#39;t exist.
## x Column `X1` doesn&#39;t exist.</code></pre>
<pre><code>## Warning: `fun.y` is deprecated. Use `fun` instead.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>From this, we might be tempted to declare a significant difference arisisng ~1500-2500 ms. However, we cannot rely on the graphical analysis and we need a statistical test to be able to affirm that this observation is caused by diffierntial effort between the two scripts.</p>
<p>Permuation cluster analysis is a technique that is becoming increasingly popular, especially in the cognitvie neuropsychology domain to analyze MEG-EEG <span class="citation">(<a href="#ref-Maris2007" role="doc-biblioref">Maris and Oostenveld 2007</a>)</span>. While there exists a number of cluster analysis functions in MNE-Python <span class="citation">(<a href="#ref-Gramfort2014" role="doc-biblioref">Gramfort et al. 2014</a>)</span> and Matlab’s FieldTrip <span class="citation">(<a href="#ref-Oostenveld2011" role="doc-biblioref">Oostenveld et al. 2011</a>)</span>, what I want to show you is how you can do this analysis in R. The implementation of CPT has not been widely used in pupillometry research. I will hopefully show you that it can be a very useful tool to have in your aresenal.</p>
<p>Before I show you how to apply this method, I want to briefly go over the CPT method. The CPT is a data-driven method which increases power and controls for Type I errors across multiple comparisons (exatcly what we need when looking at pupil changes across the time course!). The clustering method involves conducting dependent-sample t-tests for every data point (condition by time). In the first step, adjacent data points that cross the mass-univariate significance threshold (<em>p</em> &lt; .05) are combined to create a cluster. The sum of the <em>t</em>-statistic (or <em>F</em>) are calculated and form the basis for the cluster level statistic. In the second step, a surrogate null-distribution is created by first randomly assigning one of the two conditions within subjects (this is done n times) and retaining the cluster statistic for each randomization. In the third and final step, the cluster level statistic of the real comparison is compared against the null distribution, with clusters falling in the highest or lowest 2.5% considered to be significant.</p>
<p>While writing this blog post, I saw that Dale Barr presented on CPT in the context of eye-tracking. I figured I would take this opportunity to test out the packages he created.</p>
<pre class="r"><code>devtools::install_github(&quot;dalejbarr/exchangr&quot;)
devtools::install_github(&quot;dalejbarr/clusterperm&quot;)
library(exchangr)
library(clusterperm)</code></pre>
<p>The ‘clusterperm’ package has a pretty nifty <code>aov_by_bin</code> function. If we applied a multlipe comparison correction (e.g., holm) to this data.</p>
<pre class="r"><code>cur2 &lt;- aov_by_bin(agg_subject, timebins,   # clusterperm package
  aggbaseline ~ script + Error(subject))

cur2$p_adjuct&lt;-p.adjust(cur2$p, method=&quot;holm&quot;)

cur2_p=subset(cur2, cur2$p_adjuct &lt;= .05)

knitr::kable(cur2_p)</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">timebins</th>
<th align="left">effect</th>
<th align="right">stat</th>
<th align="right">p</th>
<th align="right">p_adjuct</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2000</td>
<td align="left">script</td>
<td align="right">12.97351</td>
<td align="right">0.0008456</td>
<td align="right">0.0202952</td>
</tr>
<tr class="even">
<td align="right">2100</td>
<td align="left">script</td>
<td align="right">17.08613</td>
<td align="right">0.0001720</td>
<td align="right">0.0044723</td>
</tr>
<tr class="odd">
<td align="right">2200</td>
<td align="left">script</td>
<td align="right">14.51092</td>
<td align="right">0.0004587</td>
<td align="right">0.0114670</td>
</tr>
<tr class="even">
<td align="right">2300</td>
<td align="left">script</td>
<td align="right">12.96859</td>
<td align="right">0.0008473</td>
<td align="right">0.0202952</td>
</tr>
</tbody>
</table>
<p>How does it compare to CPT?</p>
<pre class="r"><code>orig &lt;- detect_clusters_by_effect(cur2, effect, timebins, stat, p)

knitr::kable(orig)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">effect</th>
<th align="right">b0</th>
<th align="right">b1</th>
<th align="right">sign</th>
<th align="right">cms</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">script</td>
<td align="right">0</td>
<td align="right">100</td>
<td align="right">1</td>
<td align="right">10.121215</td>
</tr>
<tr class="even">
<td align="left">script</td>
<td align="right">900</td>
<td align="right">1000</td>
<td align="right">-1</td>
<td align="right">8.756152</td>
</tr>
<tr class="odd">
<td align="left">script</td>
<td align="right">1900</td>
<td align="right">2500</td>
<td align="right">1</td>
<td align="right">82.198279</td>
</tr>
</tbody>
</table>
<p>It looks like the multiple comparison correction is less sensitive.</p>
<p>Wait! we do not have <em>p</em>-values!!! Where are the damn <em>p</em>-values!? What is life? How am I suppose to make an informed decison? What is cool about Dale’s R package is that it has a permutation function that allows you to build the null model and obtain those coveted <em>p</em>-values.</p>
<pre class="r"><code>dat_prec &lt;- nest(agg_subject, -subject, -script)

nhds_prec &lt;- cluster_nhds(
  100, dat_prec, timebins,
  aggbaseline ~ script + Error(subject),
  shuffle_each, script, subject) # only use 100 permutations. Should use 1000-2000. 


## get p-values
results_prec &lt;- pvalues(orig, nhds_prec)

knitr::kable(results_prec)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">effect</th>
<th align="right">b0</th>
<th align="right">b1</th>
<th align="right">sign</th>
<th align="right">cms</th>
<th align="right">p</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">script</td>
<td align="right">0</td>
<td align="right">100</td>
<td align="right">1</td>
<td align="right">10.121215</td>
<td align="right">0.2079208</td>
</tr>
<tr class="even">
<td align="left">script</td>
<td align="right">900</td>
<td align="right">1000</td>
<td align="right">-1</td>
<td align="right">8.756152</td>
<td align="right">0.2376238</td>
</tr>
<tr class="odd">
<td align="left">script</td>
<td align="right">1900</td>
<td align="right">2500</td>
<td align="right">1</td>
<td align="right">82.198279</td>
<td align="right">0.0099010</td>
</tr>
</tbody>
</table>
<p>Whew! I feel much better now!</p>
<p>From this graph we can see that there is one signifcant cluster: from 1900-2500 ms. If you were using this as a more explortatory approach (which I think we should), we know have a time period to look at where we can perform more common tests (e.g., max or mean during that time period). We could alternatively make the inference that a difference between cursive and print-type arises somewhere around 1900-2500 ms.</p>
<pre><code>## Error: Can&#39;t subset columns that don&#39;t exist.
## x Column `X1` doesn&#39;t exist.</code></pre>
<pre><code>## Warning: `fun.y` is deprecated. Use `fun` instead.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>#Thoughts</p>
<p>There are some misunderstandings users should be made aware of when it comes to CPT <span class="citation">(see <a href="#ref-Sassenhagen2019" role="doc-biblioref">Sassenhagen and Draschkow 2019</a>)</span> for a nice discussion). If we want to ask more specific questions (i.e., at what exact time points effects arise), CPT with a cluster mass correction is not the method to use. We can only be certain that a difference exists; we cannot make claims about whether individual time points show an effect with a given error rate. I think this is more of an issue for M/EEG than pupillometry though. The pupillary signal is very slow.</p>
<p>Overall, I think CPT can be a very useful tool in determing an appropiate time range to look at, and can even lead to more powerful inferences. Further, the use of CPT obviates the need for multiple comparisons and autocorrelation corrections. However, it does have issues. For instance, it cannot take into account subject and item variability. In addition, it is not able to tell you where within the cluster there is a difference only that there is a difference. And finally, when power is low, clusters may include many false postive time points <span class="citation">(<a href="#ref-Fields2019" role="doc-biblioref">Fields and Kuperberg 2019</a>)</span>.</p>
<p>In my next blog, I will discuss applying generalized additive mixed modeling to your pupillometry data. I think it might be the perfect soultion!</p>
</div>
<div id="cited" class="section level1 unnumbered">
<h1>Cited</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Fields2019" class="csl-entry">
Fields, Eric C., and Gina R. Kuperberg. 2019. <span>“<span class="nocase">Having your cake and eating it too: Flexibility and power with mass univariate statistics for ERP data</span>.”</span> <em>Psychophysiology</em>, August. <a href="https://doi.org/10.1111/psyp.13468">https://doi.org/10.1111/psyp.13468</a>.
</div>
<div id="ref-Geller2" class="csl-entry">
Geller, Jason, Matthew Winn, Tristan Mahr, and Daniel Mirman. n.d. <span>“<span class="nocase">GazeR: A Package for Processing Gaze Position and Pupil Size Data</span>.”</span> <a href="https://doi.org/10.31234/OSF.IO/GVCXB">https://doi.org/10.31234/OSF.IO/GVCXB</a>.
</div>
<div id="ref-Gramfort2014" class="csl-entry">
Gramfort, Alexandre, Martin Luessi, Eric Larson, Denis A. Engemann, Daniel Strohmeier, Christian Brodbeck, Lauri Parkkonen, and Matti S. Hämäläinen. 2014. <span>“<span class="nocase">MNE software for processing MEG and EEG data</span>.”</span> <em>NeuroImage</em> 86 (February): 446–60. <a href="https://doi.org/10.1016/j.neuroimage.2013.10.027">https://doi.org/10.1016/j.neuroimage.2013.10.027</a>.
</div>
<div id="ref-Maris2007" class="csl-entry">
Maris, Eric, and Robert Oostenveld. 2007. <span>“<span class="nocase">Nonparametric statistical testing of EEG- and MEG-data</span>.”</span> <em>Journal of Neuroscience Methods</em> 164 (1): 177–90. <a href="https://doi.org/10.1016/j.jneumeth.2007.03.024">https://doi.org/10.1016/j.jneumeth.2007.03.024</a>.
</div>
<div id="ref-Oostenveld2011" class="csl-entry">
Oostenveld, Robert, Pascal Fries, Eric Maris, and Jan-Mathijs Schoffelen. 2011. <span>“<span class="nocase">FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data.</span>”</span> <em>Computational Intelligence and Neuroscience</em> 2011 (December): 156869. <a href="https://doi.org/10.1155/2011/156869">https://doi.org/10.1155/2011/156869</a>.
</div>
<div id="ref-Reilly2019" class="csl-entry">
Reilly, Jamie, Alexandra Kelly, Seung Hwan Kim, Savannah Jett, and Bonnie Zuckerman. 2019. <span>“<span class="nocase">The human task-evoked pupillary response function is linear: Implications for baseline response scaling in pupillometry</span>.”</span> <em>Behavior Research Methods</em> 51 (2): 865–78. <a href="https://doi.org/10.3758/s13428-018-1134-4">https://doi.org/10.3758/s13428-018-1134-4</a>.
</div>
<div id="ref-Sassenhagen2019" class="csl-entry">
Sassenhagen, Jona, and Dejan Draschkow. 2019. <span>“<span class="nocase">Cluster‐based permutation tests of MEG/EEG data do not establish significance of effect latency or location</span>.”</span> <em>Psychophysiology</em> 56 (6): e13335. <a href="https://doi.org/10.1111/psyp.13335">https://doi.org/10.1111/psyp.13335</a>.
</div>
</div>
</div>
