{
  "hash": "b371f2bca047a6e97e1bb1e38c7b8d94",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multi-Time-Point Analysis: A New Way To Analyze Pupil Data\"\ndate: 2022-03-10\ndescription: \"I walkthrough a technique called multi-time-point analysis highlighted in Yu, Chen, Yang, & Chou (2020)\"\ncategories:\n  - R\n  - eye-tracking\n  - pupil\n  - longitudinal\n  - ML\nformat:\n  html: \n    shift-heading-level-by: 1\n    include-in-header:\n      - text: |\n          <style type=\"text/css\">\n          hr.dinkus {\n              width: 50px;\n              margin: 2em auto 2em;\n              border-top: 5px dotted #454545;\n          }\n          \n          div.column-margin+hr.dinkus {\n              margin: 1em auto 2em;\n          }\n          </style>\ncitation: true\nbibliography: mtpa.bib \nexecute: \n  echo: true\n  message: false\n  warning: false\n---\n\n\n# Introduction\n\nJust a few weeks ago @Mathot released a great primer on pupillometry highlighting the method, providing guidelines on how to preprocess the data, and even offered a few suggestions on how to analyze the data (see their great figure below). This paper will for sure be something I revisit and will be the paper I send to folks interested in pupillometry. In their paper, they discuss fitting linear mixed models (LME) together with cross validation to test time points. While I think there are a lot of benefits of this approach (described in their paper), this method does not tell researchers what they really want to know: where an effect lies in the time course. In the the cross-validation + LME approach, it only takes the time points with highest *z* scores and submits those points to a final LME model. Ultimately, you can say a time point is significant at X, Y, and Z times, but cant really make any claims about the start and end of events. This got me thinking about a paper I read a few years ago by @yu_multi-time-point_2020 which looked at a method called multi-time-point analysis which they applied to fNIRS data. I really like the method (and paper) and began thinking about all the ways it could be applied (e.g., EEG, pupillometry). In the paper, they showed that it is a more powerful tool than the often used mass univariate approach. Here I show how to apply this method to pupillometry data.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](pupil_guide.PNG){fig-align='center' width=370}\n:::\n:::\n\n\n# Multi-Time Point Analysis (MTPA)\n\nThere are a wide-variety of options when deciding on an appropriate analysis strategy to use. It is almost impossible to cover every method. When reviewing papers, I feel bad because I inevitable bring up methods the authors did not include. This blog post adds to the already complicated landscape of time course analysis and add yet another tool to the pupillometry toolbox.\n\nI mostly do these blogs as an aid to help me better understand a method. I also hope it will help others.\n\nTo demonstrate MTPA I am going to use a simple example from data I collected as a postdoc at the University of Iowa. In this experiment, individuals (*N*=31) heard normal and 6-channel vocoded speech tokens (single words) and had to click on the correct picture. The vocoded condition should be harder, and in the figure below, you can see that it is--larger pupil size throughout the trial. Commonly, we would test significance by using a mass univariate approach (e.g., *t*-tests at each time point). Below we fit the time course data using Dale Barr's `clusterperm` package with no corrections (with corrections nothing is significant). We see that there is a significant difference between the conditions that emerges starting around 800 ms and ends around 1900 ms. Using MTPA would we observe something different?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pacman)\n\n#devtools::install_github(\"dalejbarr/clusterperm\")\n\n\n\npacman::p_load(ERP, mnormt, fdrtool,\n               tidyverse, gridExtra, crayon, \n               boot, reshape2, ggthemes, \n               devtools,randomForest,leaps, pROC, tidyverse, here, gazer, clusterperm)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvo_mu <- aov_by_bin(timebins1, timebins,   # clusterperm package\n  aggbaseline ~ vocoded + Error(subject))\n\nvo_mu$p_adjuct<-p.adjust(vo_mu$p, method=\"none\")\n\nvo_mu_p=subset(vo_mu, vo_mu$p_adjuct <= .05)\n\nknitr::kable(vo_mu_p)\n```\n\n::: {.cell-output-display}\n\n\n| timebins|effect  |       stat|         p|  p_adjuct|\n|--------:|:-------|----------:|---------:|---------:|\n|      800|vocoded |  -6.729464| 0.0145280| 0.0145280|\n|      900|vocoded |  -7.085248| 0.0123675| 0.0123675|\n|     1000|vocoded |  -6.560462| 0.0156954| 0.0156954|\n|     1100|vocoded |  -8.561962| 0.0064854| 0.0064854|\n|     1200|vocoded |  -9.519850| 0.0043426| 0.0043426|\n|     1300|vocoded |  -9.398130| 0.0045664| 0.0045664|\n|     1400|vocoded | -10.524933| 0.0028890| 0.0028890|\n|     1500|vocoded | -11.652584| 0.0018560| 0.0018560|\n|     1600|vocoded | -11.458565| 0.0020007| 0.0020007|\n|     1700|vocoded | -11.172190| 0.0022369| 0.0022369|\n|     1800|vocoded |  -9.103745| 0.0051610| 0.0051610|\n|     1900|vocoded |  -5.687212| 0.0236102| 0.0236102|\n\n\n:::\n:::\n\n\n# Before We Begin: A Summary\n\nBefore jumping into the MTPA analysis, I want to explain the method a little bit. The MTPA uses a random forest classifier in conjunction with cross validation to determine if there is a statistical difference in the pupil signal between conditions. This differs slightly from the approach outlined in [@Mathot]. In their approach, linear mixed models are used in conjunction with cross validation to pick samples with the highest Z value. In contrast, MTPA uses all time points. What MTPA does is calculate the area under the curve (AUC) along with 90% confidence intervals and averages them. Significance is determined by whether the CIs cross the .5 threshold. If it does not, there is a difference between conditions at that time point. Below I will outline specific steps needed to perform MTPA along with the R code.\n\n# Running MPTA in R\n\n1.  We need to read in our sample data (described above) and `pivot_wide` so time is in columns and subjects and conditions are long.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Read Data\n# Read the Data preprocessed by gazer \nvocode<- read.csv(\"mtpa_file.csv\")\n\nvocode_wide<- vocode %>%\n  select(-vocoded, -X) %>% \n  group_by(subject, Condition) %>%\n  #add T1:27 for time bc weird things when cols are numeric turn condition into factor\n mutate(timebins=rep(paste(\"T\", 1:27, sep=\"\"),by=length(timebins)), Condition=as.factor(Condition)) %>%\n    mutate(expt=\"pupil\", Subject=as.factor(subject), Condition=ifelse(Condition==\"NS\", 0, 1), Condition=as.factor(Condition)) %>%\n  ungroup() %>%\n  select(-subject) %>%\n  pivot_wider(names_from = timebins, values_from = aggbaseline) %>%\n  as.data.frame() %>%\n  arrange(Condition)%>%\n  datawizard::data_reorder(c(\"expt\", \"Subject\", \"Condition\")) %>%\n  ungroup()\n```\n:::\n\n\n2.  In MTPA, we must first partition the data into bandwidths(here two), or the number of points to consider at a time in the analysisâ€”-for example with a bandwidth of 2, a model would be built from two time points (e.g., 1 and 2 and then 2 and 3 and so on and so forth until the last time point has been fit). In the matrix below, we see that category membership (Related (R) vs. Unrelated (U), or in our case normal and vocoded speech) is predicted from the signal at two time points. This is repeated until the last time point. In our example, this is the 27th time point, or 2500 ms. As a note, @yu_multi-time-point_2020 recommend 2 and stated other widths did not result in different conclusions.\n\n\n::: {.cell layout-align=\"center\" Message='false'}\n::: {.cell-output-display}\n![](matrix.png){fig-align='center' width=215}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define time point starts at -100 and ends at 2500\ntp <- seq(-100,2500,by=100)\n\n### Start MTPA\n### Set parameters for MTPA\n# Consider 2 time points at each testing\nbinwidth = 2\n```\n:::\n\n\n3.  After this, we set our number of cross validations (100 times here), confidence interval, and our upper (number of time points (here it is 27) - banwidth (2) + 1)) and lower bounds, and create a matrix to store important model results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrcvnum <- 100\n# Confidence interval\nci <- c(0.05,0.95)\n# Set the upper and lower bound \n# number of timepoint - bin + 1\nupperbound <- 27-binwidth+1\nlowerbound <- 1\n# Store the results\nrst_vocode <- matrix(NA,6,upperbound)\n```\n:::\n\n\n4.  For every time point X CV, we fit a random forest model using the training data we created and use the test set for prediction. It is suggested that you sample 60\\~70% (two-thirds) of the data to train the model, and use the rest 30\\~40% (one-third) as your testing set. AUC and other important metrics are calculated for each time point and averaged together and stored in a matrix before going to the next time point. In our example we make sure that an equal number of normal speech and vocoded speech conditions are in each training and test sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Start MTPA model fitting with RF\nfor (i in lowerbound:upperbound){\n  # Record the AUC and CE\n  ceauc <- matrix(NA,4,rcvnum)\n  # Start cross validation\n  for (k in 1:rcvnum){\n    # Set seed for reproducible research\n    set.seed(k)\n    # Training and Testing Data\n    idc_test <- c(sample(1:30,5),sample(31:62,5))\n    idc_train <- -idc_test\n    # Fit an RF model\n    fit <- randomForest(Condition~.,data=vocode_wide[idc_train,c(2,3,(i+3):(i+3+binwidth-1))],importance = F)\n    yhat_test_prob <- predict(fit,newdata = vocode_wide[idc_test,],type = \"prob\")[,2]\n    yhat_test_class <- predict(fit,newdata = vocode_wide[idc_test,],type = \"class\")\n    # Record the results of RF fitting on Testing data\n    ce_test <- mean(yhat_test_class!=vocode_wide[idc_test,]$Condition)\n    auc_test <- pROC::auc(vocode_wide[idc_test,]$Condition,yhat_test_prob)\n    ceauc[2,k] <- ce_test\n    ceauc[4,k] <- auc_test\n  }\n  # Store the results of CV\n  rst_vocode[1,i] <- mean(ceauc[2,])\n  rst_vocode[2,i] <- mean(ceauc[4,])\n  rst_vocode[3,i] <- quantile(ceauc[2,],probs = c(ci[1],ci[2]))[1]\n  rst_vocode[4,i] <- quantile(ceauc[2,],probs = c(ci[1],ci[2]))[2]\n  rst_vocode[5,i] <- quantile(ceauc[4,],probs = c(ci[1],ci[2]))[1]\n  rst_vocode[6,i] <- quantile(ceauc[4,],probs = c(ci[1],ci[2]))[2]\n}\n# Reorganize the results, average all the time points that used to estimate the results\nvocodem <- matrix(NA,6,27)\nvocodem[,1] <- rst_vocode[,1]\nvocodem[,27] <- rst_vocode[,26]\nfor (i in 1:(upperbound-1)){\n  tpi <- i + 1\n  vocodem[1,tpi] <- mean(rst_vocode[1,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[2,tpi] <- mean(rst_vocode[2,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[3,tpi] <- mean(rst_vocode[3,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[4,tpi] <- mean(rst_vocode[4,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[5,tpi] <- mean(rst_vocode[5,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[6,tpi] <- mean(rst_vocode[6,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n}\nvocodem <- as.data.frame(vocodem) # turn into df \ncolnames(vocodem) <- paste0(\"Time\",1:27) #label time\nrow.names(vocodem) <- c(\"CE\",\"AUC\",\"CE_l\",\"CE_u\",\"AUC_l\",\"AUC_u\") # label metrics\n```\n:::\n\n\n5.  Finally we transpose the matrix, put time in ms, and plot the AUC at each time point. Where the CIs do not cross the 50% threshold (red dotted line), a significant difference can be said to exist at that time point between conditions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- as.data.frame(t(vocodem)) # transpose\ntemp$Times <- tp # turn time into ms \n\n# plot\nggplot(data = temp,aes(x =Times, y = AUC))+\n  geom_line(size = 1.2)+\n  geom_ribbon(aes(ymax = AUC_l,ymin = AUC_u),alpha = 0.3)+\n  theme_bw() + \n  coord_cartesian(ylim = c(.4, 1)) + \ngeom_hline(yintercept=.5, linetype=\"dashed\", \n                color = \"red\", size=2) + \n  labs(x=\"Time(ms)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n# Conclusions\n\nI have shown how to apply MTPA to pupillometric data. This method appears to be more powerful than a mass univariate approach, showing the whole time course as significant. It would be interesting to see how this compares to the method proposed by @Mathot. I would encourage anyone who is interested in applying this method to also check out @yu_multi-time-point_2020's excellent paper which this blog is based off of.\n\n## Potential Limitations\n\nWhile this method looks promising, there does appear to be some limitations. First, it look looks like you can't fit multilevel models. If you can, I suspect it is 1) computationally expensive and 2) not trivial. If anyone has an answer to this I would love to hear it. Second, it appears that you would have to conduct this test for each effect of interest rather then including everything in the model. This is something that @Mathot also brings up in their paper. I will have to do some more investigating, but overall I am quite interested in learning more about ML applications to time course data like pupillometry.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] clusterperm_0.1.0.9000 gazer_0.1              here_1.0.1            \n [4] pROC_1.18.5            leaps_3.1              randomForest_4.7-1.1  \n [7] devtools_2.4.5         usethis_2.2.3          ggthemes_5.1.0        \n[10] reshape2_1.4.4         boot_1.3-30            crayon_1.5.2          \n[13] gridExtra_2.3          lubridate_1.9.3        forcats_1.0.0         \n[16] stringr_1.5.1          dplyr_1.1.4            purrr_1.0.2           \n[19] readr_2.1.5            tidyr_1.3.1            tibble_3.2.1          \n[22] ggplot2_3.5.1          tidyverse_2.0.0        fdrtool_1.2.17        \n[25] mnormt_2.1.1           ERP_2.2                pacman_0.5.1          \n[28] papaja_0.1.2           tinylabels_0.2.4       knitr_1.46            \n\nloaded via a namespace (and not attached):\n [1] remotes_2.5.0     rlang_1.1.4       magrittr_2.0.3    compiler_4.4.0   \n [5] png_0.1-8         vctrs_0.6.5       profvis_0.3.8     pkgconfig_2.0.3  \n [9] fastmap_1.1.1     backports_1.5.0   ellipsis_0.3.2    labeling_0.4.3   \n[13] effectsize_0.8.7  utf8_1.2.4        promises_1.3.0    rmarkdown_2.26   \n[17] sessioninfo_1.2.2 tzdb_0.4.0        bit_4.0.5         xfun_0.43        \n[21] cachem_1.0.8      jsonlite_1.8.8    later_1.3.2       broom_1.0.6      \n[25] irlba_2.3.5.1     parallel_4.4.0    R6_2.5.1          stringi_1.8.4    \n[29] pkgload_1.3.4     estimability_1.5  Rcpp_1.0.12       zoo_1.8-12       \n[33] parameters_0.21.6 httpuv_1.6.15     Matrix_1.7-0      splines_4.4.0    \n[37] timechange_0.3.0  tidyselect_1.2.1  rstudioapi_0.16.0 yaml_2.3.8       \n[41] miniUI_0.1.1.1    pkgbuild_1.4.4    lattice_0.22-6    plyr_1.8.9       \n[45] shiny_1.8.1.1     withr_3.0.0       bayestestR_0.13.2 coda_0.19-4.1    \n[49] evaluate_0.23     urlchecker_1.0.1  pillar_1.9.0      insight_0.19.10  \n[53] generics_0.1.3    vroom_1.6.5       rprojroot_2.0.4   hms_1.1.3        \n[57] munsell_0.5.1     scales_1.3.0      xtable_1.8-4      glue_1.7.0       \n[61] emmeans_1.10.1    tools_4.4.0       data.table_1.15.4 fs_1.6.4         \n[65] mvtnorm_1.2-4     grid_4.4.0        datawizard_0.10.0 colorspace_2.1-0 \n[69] Rmisc_1.5.1       cli_3.6.2         fansi_1.0.6       corpcor_1.6.10   \n[73] gtable_0.3.5      digest_0.6.35     farver_2.1.1      htmlwidgets_1.6.4\n[77] memoise_2.0.1     htmltools_0.5.8.1 lifecycle_1.0.4   mime_0.12        \n[81] bit64_4.0.5      \n```\n\n\n:::\n:::\n\n\n# References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}