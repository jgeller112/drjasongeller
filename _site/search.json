[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data\n\n\n\n\n\n\nR\n\n\neye-tracking\n\n\npupil\n\n\nlongitudinal\n\n\nML\n\n\n\nI walkthrough a technique called multi-time-point analysis highlighted in Yu, Chen, Yang, & Chou (2020)\n\n\n\n\n\nMarch 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnalzying GazePoint Pupil Data With GazeR\n\n\n\n\n\n\nr\n\n\npupillometry\n\n\nstatistics\n\n\n\nI demonstrate how to analyze pupil data from a GazePoint tracker with my eye-tracking R package gazeR.\n\n\n\n\n\nApril 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMass Univariate Analysis for Pupillometric Data: Cluster Permutation Test\n\n\n\n\n\n\nr\n\n\npupillometry\n\n\nstatistics\n\n\n\nI demonstrate how to analyze pupil data using a cluster based permutation test\n\n\n\n\n\nJuly 10, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2022-03-10-mtpa-pupil/index.html",
    "href": "blog/posts/2022-03-10-mtpa-pupil/index.html",
    "title": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data",
    "section": "",
    "text": "Just a few weeks ago Math√¥t & Vilotijeviƒá (2022) released a great primer on pupillometry highlighting the method, providing guidelines on how to preprocess the data, and even offered a few suggestions on how to analyze the data (see their great figure below). This paper will for sure be something I revisit and will be the paper I send to folks interested in pupillometry. In their paper, they discuss fitting linear mixed models (LME) together with cross validation to test time points. While I think there are a lot of benefits of this approach (described in their paper), this method does not tell researchers what they really want to know: where an effect lies in the time course. In the the cross-validation + LME approach, it only takes the time points with highest z scores and submits those points to a final LME model. Ultimately, you can say a time point is significant at X, Y, and Z times, but cant really make any claims about the start and end of events. This got me thinking about a paper I read a few years ago by Yu et al. (2020) which looked at a method called multi-time-point analysis which they applied to fNIRS data. I really like the method (and paper) and began thinking about all the ways it could be applied (e.g., EEG, pupillometry). In the paper, they showed that it is a more powerful tool than the often used mass univariate approach. Here I show how to apply this method to pupillometry data."
  },
  {
    "objectID": "blog/posts/2022-03-10-mtpa-pupil/index.html#introduction",
    "href": "blog/posts/2022-03-10-mtpa-pupil/index.html#introduction",
    "title": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data",
    "section": "",
    "text": "Just a few weeks ago Math√¥t & Vilotijeviƒá (2022) released a great primer on pupillometry highlighting the method, providing guidelines on how to preprocess the data, and even offered a few suggestions on how to analyze the data (see their great figure below). This paper will for sure be something I revisit and will be the paper I send to folks interested in pupillometry. In their paper, they discuss fitting linear mixed models (LME) together with cross validation to test time points. While I think there are a lot of benefits of this approach (described in their paper), this method does not tell researchers what they really want to know: where an effect lies in the time course. In the the cross-validation + LME approach, it only takes the time points with highest z scores and submits those points to a final LME model. Ultimately, you can say a time point is significant at X, Y, and Z times, but cant really make any claims about the start and end of events. This got me thinking about a paper I read a few years ago by Yu et al. (2020) which looked at a method called multi-time-point analysis which they applied to fNIRS data. I really like the method (and paper) and began thinking about all the ways it could be applied (e.g., EEG, pupillometry). In the paper, they showed that it is a more powerful tool than the often used mass univariate approach. Here I show how to apply this method to pupillometry data."
  },
  {
    "objectID": "blog/posts/2022-03-10-mtpa-pupil/index.html#multi-time-point-analysis-mtpa",
    "href": "blog/posts/2022-03-10-mtpa-pupil/index.html#multi-time-point-analysis-mtpa",
    "title": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data",
    "section": "Multi-Time Point Analysis (MTPA)",
    "text": "Multi-Time Point Analysis (MTPA)\nThere are a wide-variety of options when deciding on an appropriate analysis strategy to use. It is almost impossible to cover every method. When reviewing papers, I feel bad because I inevitable bring up methods the authors did not include. This blog post adds to the already complicated landscape of time course analysis and add yet another tool to the pupillometry toolbox.\nI mostly do these blogs as an aid to help me better understand a method. I also hope it will help others.\nTo demonstrate MTPA I am going to use a simple example from data I collected as a postdoc at the University of Iowa. In this experiment, individuals (N=31) heard normal and 6-channel vocoded speech tokens (single words) and had to click on the correct picture. The vocoded condition should be harder, and in the figure below, you can see that it is‚Äìlarger pupil size throughout the trial. Commonly, we would test significance by using a mass univariate approach (e.g., t-tests at each time point). Below we fit the time course data using Dale Barr‚Äôs clusterperm package with no corrections (with corrections nothing is significant). We see that there is a significant difference between the conditions that emerges starting around 800 ms and ends around 1900 ms. Using MTPA would we observe something different?\n\nlibrary(pacman)\n\n#devtools::install_github(\"dalejbarr/clusterperm\")\n\n\n\npacman::p_load(ERP, mnormt, fdrtool,\n               tidyverse, gridExtra, crayon, \n               boot, reshape2, ggthemes, \n               devtools,randomForest,leaps, pROC, tidyverse, here, gazer, clusterperm)\n\n\n\n\n\n\n\n\n\n\nvo_mu &lt;- aov_by_bin(timebins1, timebins,   # clusterperm package\n  aggbaseline ~ vocoded + Error(subject))\n\nvo_mu$p_adjuct&lt;-p.adjust(vo_mu$p, method=\"none\")\n\nvo_mu_p=subset(vo_mu, vo_mu$p_adjuct &lt;= .05)\n\nknitr::kable(vo_mu_p)\n\n\n\ntimebins\neffect\nstat\np\np_adjuct\n\n\n\n800\nvocoded\n-6.729464\n0.0145280\n0.0145280\n\n\n900\nvocoded\n-7.085248\n0.0123675\n0.0123675\n\n\n1000\nvocoded\n-6.560462\n0.0156954\n0.0156954\n\n\n1100\nvocoded\n-8.561962\n0.0064854\n0.0064854\n\n\n1200\nvocoded\n-9.519850\n0.0043426\n0.0043426\n\n\n1300\nvocoded\n-9.398130\n0.0045664\n0.0045664\n\n\n1400\nvocoded\n-10.524933\n0.0028890\n0.0028890\n\n\n1500\nvocoded\n-11.652584\n0.0018560\n0.0018560\n\n\n1600\nvocoded\n-11.458565\n0.0020007\n0.0020007\n\n\n1700\nvocoded\n-11.172190\n0.0022369\n0.0022369\n\n\n1800\nvocoded\n-9.103745\n0.0051610\n0.0051610\n\n\n1900\nvocoded\n-5.687212\n0.0236102\n0.0236102"
  },
  {
    "objectID": "blog/posts/2022-03-10-mtpa-pupil/index.html#before-we-begin-a-summary",
    "href": "blog/posts/2022-03-10-mtpa-pupil/index.html#before-we-begin-a-summary",
    "title": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data",
    "section": "Before We Begin: A Summary",
    "text": "Before We Begin: A Summary\nBefore jumping into the MTPA analysis, I want to explain the method a little bit. The MTPA uses a random forest classifier in conjunction with cross validation to determine if there is a statistical difference in the pupil signal between conditions. This differs slightly from the approach outlined in (Math√¥t & Vilotijeviƒá, 2022). In their approach, linear mixed models are used in conjunction with cross validation to pick samples with the highest Z value. In contrast, MTPA uses all time points. What MTPA does is calculate the area under the curve (AUC) along with 90% confidence intervals and averages them. Significance is determined by whether the CIs cross the .5 threshold. If it does not, there is a difference between conditions at that time point. Below I will outline specific steps needed to perform MTPA along with the R code."
  },
  {
    "objectID": "blog/posts/2022-03-10-mtpa-pupil/index.html#running-mpta-in-r",
    "href": "blog/posts/2022-03-10-mtpa-pupil/index.html#running-mpta-in-r",
    "title": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data",
    "section": "Running MPTA in R",
    "text": "Running MPTA in R\n\nWe need to read in our sample data (described above) and pivot_wide so time is in columns and subjects and conditions are long.\n\n\n### Read Data\n# Read the Data preprocessed by gazer \nvocode&lt;- read.csv(\"mtpa_file.csv\")\n\nvocode_wide&lt;- vocode %&gt;%\n  select(-vocoded, -X) %&gt;% \n  group_by(subject, Condition) %&gt;%\n  #add T1:27 for time bc weird things when cols are numeric turn condition into factor\n mutate(timebins=rep(paste(\"T\", 1:27, sep=\"\"),by=length(timebins)), Condition=as.factor(Condition)) %&gt;%\n    mutate(expt=\"pupil\", Subject=as.factor(subject), Condition=ifelse(Condition==\"NS\", 0, 1), Condition=as.factor(Condition)) %&gt;%\n  ungroup() %&gt;%\n  select(-subject) %&gt;%\n  pivot_wider(names_from = timebins, values_from = aggbaseline) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(Condition)%&gt;%\n  datawizard::data_reorder(c(\"expt\", \"Subject\", \"Condition\")) %&gt;%\n  ungroup()\n\n\nIn MTPA, we must first partition the data into bandwidths(here two), or the number of points to consider at a time in the analysis‚Äî-for example with a bandwidth of 2, a model would be built from two time points (e.g., 1 and 2 and then 2 and 3 and so on and so forth until the last time point has been fit). In the matrix below, we see that category membership (Related (R) vs.¬†Unrelated (U), or in our case normal and vocoded speech) is predicted from the signal at two time points. This is repeated until the last time point. In our example, this is the 27th time point, or 2500 ms. As a note, Yu et al. (2020) recommend 2 and stated other widths did not result in different conclusions.\n\n\n\n\n\n\n\n\n\n\n# Define time point starts at -100 and ends at 2500\ntp &lt;- seq(-100,2500,by=100)\n\n### Start MTPA\n### Set parameters for MTPA\n# Consider 2 time points at each testing\nbinwidth = 2\n\n\nAfter this, we set our number of cross validations (100 times here), confidence interval, and our upper (number of time points (here it is 27) - banwidth (2) + 1)) and lower bounds, and create a matrix to store important model results.\n\n\nrcvnum &lt;- 100\n# Confidence interval\nci &lt;- c(0.05,0.95)\n# Set the upper and lower bound \n# number of timepoint - bin + 1\nupperbound &lt;- 27-binwidth+1\nlowerbound &lt;- 1\n# Store the results\nrst_vocode &lt;- matrix(NA,6,upperbound)\n\n\nFor every time point X CV, we fit a random forest model using the training data we created and use the test set for prediction. It is suggested that you sample 60~70% (two-thirds) of the data to train the model, and use the rest 30~40% (one-third) as your testing set. AUC and other important metrics are calculated for each time point and averaged together and stored in a matrix before going to the next time point. In our example we make sure that an equal number of normal speech and vocoded speech conditions are in each training and test sample.\n\n\n### Start MTPA model fitting with RF\nfor (i in lowerbound:upperbound){\n  # Record the AUC and CE\n  ceauc &lt;- matrix(NA,4,rcvnum)\n  # Start cross validation\n  for (k in 1:rcvnum){\n    # Set seed for reproducible research\n    set.seed(k)\n    # Training and Testing Data\n    idc_test &lt;- c(sample(1:30,5),sample(31:62,5))\n    idc_train &lt;- -idc_test\n    # Fit an RF model\n    fit &lt;- randomForest(Condition~.,data=vocode_wide[idc_train,c(2,3,(i+3):(i+3+binwidth-1))],importance = F)\n    yhat_test_prob &lt;- predict(fit,newdata = vocode_wide[idc_test,],type = \"prob\")[,2]\n    yhat_test_class &lt;- predict(fit,newdata = vocode_wide[idc_test,],type = \"class\")\n    # Record the results of RF fitting on Testing data\n    ce_test &lt;- mean(yhat_test_class!=vocode_wide[idc_test,]$Condition)\n    auc_test &lt;- pROC::auc(vocode_wide[idc_test,]$Condition,yhat_test_prob)\n    ceauc[2,k] &lt;- ce_test\n    ceauc[4,k] &lt;- auc_test\n  }\n  # Store the results of CV\n  rst_vocode[1,i] &lt;- mean(ceauc[2,])\n  rst_vocode[2,i] &lt;- mean(ceauc[4,])\n  rst_vocode[3,i] &lt;- quantile(ceauc[2,],probs = c(ci[1],ci[2]))[1]\n  rst_vocode[4,i] &lt;- quantile(ceauc[2,],probs = c(ci[1],ci[2]))[2]\n  rst_vocode[5,i] &lt;- quantile(ceauc[4,],probs = c(ci[1],ci[2]))[1]\n  rst_vocode[6,i] &lt;- quantile(ceauc[4,],probs = c(ci[1],ci[2]))[2]\n}\n# Reorganize the results, average all the time points that used to estimate the results\nvocodem &lt;- matrix(NA,6,27)\nvocodem[,1] &lt;- rst_vocode[,1]\nvocodem[,27] &lt;- rst_vocode[,26]\nfor (i in 1:(upperbound-1)){\n  tpi &lt;- i + 1\n  vocodem[1,tpi] &lt;- mean(rst_vocode[1,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[2,tpi] &lt;- mean(rst_vocode[2,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[3,tpi] &lt;- mean(rst_vocode[3,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[4,tpi] &lt;- mean(rst_vocode[4,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[5,tpi] &lt;- mean(rst_vocode[5,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n  vocodem[6,tpi] &lt;- mean(rst_vocode[6,c((i+lowerbound-1):(i+lowerbound-1+binwidth-1))])\n}\nvocodem &lt;- as.data.frame(vocodem) # turn into df \ncolnames(vocodem) &lt;- paste0(\"Time\",1:27) #label time\nrow.names(vocodem) &lt;- c(\"CE\",\"AUC\",\"CE_l\",\"CE_u\",\"AUC_l\",\"AUC_u\") # label metrics\n\n\nFinally we transpose the matrix, put time in ms, and plot the AUC at each time point. Where the CIs do not cross the 50% threshold (red dotted line), a significant difference can be said to exist at that time point between conditions.\n\n\ntemp &lt;- as.data.frame(t(vocodem)) # transpose\ntemp$Times &lt;- tp # turn time into ms \n\n# plot\nggplot(data = temp,aes(x =Times, y = AUC))+\n  geom_line(size = 1.2)+\n  geom_ribbon(aes(ymax = AUC_l,ymin = AUC_u),alpha = 0.3)+\n  theme_bw() + \n  coord_cartesian(ylim = c(.4, 1)) + \ngeom_hline(yintercept=.5, linetype=\"dashed\", \n                color = \"red\", size=2) + \n  labs(x=\"Time(ms)\")"
  },
  {
    "objectID": "blog/posts/2022-03-10-mtpa-pupil/index.html#conclusions",
    "href": "blog/posts/2022-03-10-mtpa-pupil/index.html#conclusions",
    "title": "Multi-Time-Point Analysis: A New Way To Analyze Pupil Data",
    "section": "Conclusions",
    "text": "Conclusions\nI have shown how to apply MTPA to pupillometric data. This method appears to be more powerful than a mass univariate approach, showing the whole time course as significant. It would be interesting to see how this compares to the method proposed by Math√¥t & Vilotijeviƒá (2022). I would encourage anyone who is interested in applying this method to also check out Yu et al. (2020)‚Äôs excellent paper which this blog is based off of.\nPotential Limitations\nWhile this method looks promising, there does appear to be some limitations. First, it look looks like you can‚Äôt fit multilevel models. If you can, I suspect it is 1) computationally expensive and 2) not trivial. If anyone has an answer to this I would love to hear it. Second, it appears that you would have to conduct this test for each effect of interest rather then including everything in the model. This is something that Math√¥t & Vilotijeviƒá (2022) also brings up in their paper. I will have to do some more investigating, but overall I am quite interested in learning more about ML applications to time course data like pupillometry.\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] clusterperm_0.1.0.9000 gazer_0.1              here_1.0.1            \n [4] pROC_1.18.5            leaps_3.1              randomForest_4.7-1.1  \n [7] devtools_2.4.5         usethis_2.2.3          ggthemes_5.1.0        \n[10] reshape2_1.4.4         boot_1.3-30            crayon_1.5.2          \n[13] gridExtra_2.3          lubridate_1.9.3        forcats_1.0.0         \n[16] stringr_1.5.1          dplyr_1.1.4            purrr_1.0.2           \n[19] readr_2.1.5            tidyr_1.3.1            tibble_3.2.1          \n[22] ggplot2_3.5.1          tidyverse_2.0.0        fdrtool_1.2.17        \n[25] mnormt_2.1.1           ERP_2.2                pacman_0.5.1          \n[28] papaja_0.1.2           tinylabels_0.2.4       knitr_1.46            \n\nloaded via a namespace (and not attached):\n [1] remotes_2.5.0     rlang_1.1.4       magrittr_2.0.3    compiler_4.4.0   \n [5] png_0.1-8         vctrs_0.6.5       profvis_0.3.8     pkgconfig_2.0.3  \n [9] fastmap_1.1.1     backports_1.5.0   ellipsis_0.3.2    labeling_0.4.3   \n[13] effectsize_0.8.7  utf8_1.2.4        promises_1.3.0    rmarkdown_2.26   \n[17] sessioninfo_1.2.2 tzdb_0.4.0        bit_4.0.5         xfun_0.43        \n[21] cachem_1.0.8      jsonlite_1.8.8    later_1.3.2       broom_1.0.6      \n[25] irlba_2.3.5.1     parallel_4.4.0    R6_2.5.1          stringi_1.8.4    \n[29] pkgload_1.3.4     estimability_1.5  Rcpp_1.0.12       zoo_1.8-12       \n[33] parameters_0.21.6 httpuv_1.6.15     Matrix_1.7-0      splines_4.4.0    \n[37] timechange_0.3.0  tidyselect_1.2.1  rstudioapi_0.16.0 yaml_2.3.8       \n[41] miniUI_0.1.1.1    pkgbuild_1.4.4    lattice_0.22-6    plyr_1.8.9       \n[45] shiny_1.8.1.1     withr_3.0.0       bayestestR_0.13.2 coda_0.19-4.1    \n[49] evaluate_0.23     urlchecker_1.0.1  pillar_1.9.0      insight_0.19.10  \n[53] generics_0.1.3    vroom_1.6.5       rprojroot_2.0.4   hms_1.1.3        \n[57] munsell_0.5.1     scales_1.3.0      xtable_1.8-4      glue_1.7.0       \n[61] emmeans_1.10.1    tools_4.4.0       data.table_1.15.4 fs_1.6.4         \n[65] mvtnorm_1.2-4     grid_4.4.0        datawizard_0.10.0 colorspace_2.1-0 \n[69] Rmisc_1.5.1       cli_3.6.2         fansi_1.0.6       corpcor_1.6.10   \n[73] gtable_0.3.5      digest_0.6.35     farver_2.1.1      htmlwidgets_1.6.4\n[77] memoise_2.0.1     htmltools_0.5.8.1 lifecycle_1.0.4   mime_0.12        \n[81] bit64_4.0.5"
  },
  {
    "objectID": "Software/packages/gazer.html",
    "href": "Software/packages/gazer.html",
    "title": "gazeR: A package for processing gaze position and pupil size data",
    "section": "",
    "text": "üêô GitHub:Github Repository\nüìÑ Documentation: Read the paper\n\n\n\nCitationBibTeX citation:@online{geller,\n  author = {Geller, Jason and Mirman, Daniel and Mahr, Tristan and B.\n    Winn, Matt},\n  title = {gazeR: {A} Package for Processing Gaze Position and Pupil\n    Size Data},\n  url = {https://github.com/dmirman/gazer},\n  langid = {en},\n  abstract = {gazeR is an R package for processing and analyzing\n    eye-tracking data (gaze and pupil size). This package contains pupil\n    and gaze preprocessing functions}\n}\nFor attribution, please cite this work as:\nGeller, J., Mirman, D., Mahr, T., & B. Winn, M. (n.d.). gazeR: A\npackage for processing gaze position and pupil size data. https://github.com/dmirman/gazer"
  },
  {
    "objectID": "Software/index.html",
    "href": "Software/index.html",
    "title": "Software",
    "section": "",
    "text": "This page highlights software packages I‚Äôve developed to support open and reproducible research, particularly in eye-tracking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngazeR: A package for processing gaze position and pupil size data\n\n\ngazeR is an R package for processing and analyzing eye-tracking data (gaze and pupil size). This package contains pupil and gaze preprocessing functions\n\n\n\nJason Geller, Daniel Mirman, Tristan Mahr, Matt B. Winn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwebgazeR\n\n\nwebgazeR is a webcam eye-tracking toolkit for pre-processing webcam gaze data\n\n\n\nJason Geller\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "A little about me",
    "section": "",
    "text": "Thanks for stopping by!\nI am currently the Director of the Human Neuroscience Lab at Boston College. My research program investigates the cognitive and neurological underpinnings of language processing, with a particular focus on the mechanisms underlying visual word recognition and speech perception. On the weekends, I turn to my other passion: human memory. In this area, I am especially interested in semantics, metacognition and metamemory, and the role of desirable difficulties in learning. Additionally, I have written on methods and statistics in psychological science, with interests that include intensive longitudinal data analysis (time series), large-team science, non-normal data analysis, and multilevel modeling.\nMy philosophy is that cognition is best understood by examining both behavior and the brain. To gain deeper insights into cognitive processes, I employ a multi-method approach that integrates standard behavioral measurements (e.g., reaction times), eye-tracking, neuroimaging, and physiological and neurophysiological indices (e.g., pupillometry, EEG/sEEG).\nEqually important, my research is grounded in the principles of open science. I am committed to transparency and reproducibility, ensuring that all my work is accessible and replicable."
  },
  {
    "objectID": "Research/preprints/2025-beta/index.html",
    "href": "Research/preprints/2025-beta/index.html",
    "title": "A Beta Way: A tutorial for using Beta regression in psychological research",
    "section": "",
    "text": "Abstract\nRates, percentages, and proportions are common outcomes in psychology and the social sciences. These outcomes are often analyzed using models that assume normality, but this practice overlooks important features of the data, such as their natural bounds at 0 and 1. As a result, estimates can become distorted. In contrast, treating such outcomes as Beta-distributed respects these limits and can yield more accurate estimates. Despite these advantages, the use of Beta models in applied research remains limited. Our goal is to provide researchers with practical guidance for adopting Beta regression models, illustrated with an example drawn from the psychological literature. We begin by introducing the Beta distribution and Beta regression, emphasizing key components and assumptions. Next, using data from a learning and memory study, we demonstrate how to fit a Beta regression model in R with the Bayesian package brms and how to interpret results on the response scale. We also discuss model extensions, including zero-inflated, zero- and one-inflated, and ordered Beta models. Basic familarity with regression modeling and R is assumed. To promote wider adoption of these methods, we provide detailed code and materials at https://github.com/jgeller112/Beta_regression_tutorial."
  },
  {
    "objectID": "Research/index.html",
    "href": "Research/index.html",
    "title": "Publications",
    "section": "",
    "text": "This is home for my research publications, each is linked with PDFs, preprints, and corresponding repositories with data and code, if applicable. Please reach out if there is something you want that you cannot obtain."
  },
  {
    "objectID": "Research/index.html#preprints",
    "href": "Research/index.html#preprints",
    "title": "Publications",
    "section": "Preprints",
    "text": "Preprints\n\n\n\n    \n        \n            \n                Geller, J., Kubinec, R, Parlett, C., and Vuorre, M.(2025). A Beta way: A tutorial for using Beta regression in psychological research. https://doi.org/10.31234/osf.io/d6v5t_v3\n            \n\n            \n            \n                \n                    \n                            beta regression\n                        \n                    \n                    \n                            beta distribution\n                        \n                    \n                    \n                            R tutorial\n                        \n                    \n                    \n                            psychology\n                        \n                    \n                    \n                            learning and memory\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint/HTML\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Github\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Nadler, E., Geller, J., Bers, M.U.(2025). The coding brain: P600 effects elicited by the visual programming language ScratchJr. https://doi.org/10.31234/osf.io/zy8d5_v1\n            \n\n            \n            \n                \n                    \n                            Computer programming\n                        \n                    \n                    \n                            P600\n                        \n                    \n                    \n                            N400\n                        \n                    \n                    \n                            ScratchJr\n                        \n                    \n                    \n                            Semantic processing\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Colab\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "Research/index.html#journal-articles",
    "href": "Research/index.html#journal-articles",
    "title": "Publications",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Geller, J., Gomez, P., Buchanon, E., & Makowski, D. (2025).A response time distributional analysis of the perceptual disfluency effect.Journal of Cognition.https://osf.io/preprints/psyarxiv/sae23_v3\n            \n\n            \n            \n                \n                    \n                            Disfluency\n                        \n                    \n                    \n                            LDT\n                        \n                    \n                    \n                            DDM\n                        \n                    \n                    \n                            ex-Gaussian\n                        \n                    \n                    \n                            Distributional analyses\n                        \n                    \n                    \n                            Word recognition\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Github\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Buchanan, E. M., Elsherif, M. M., Geller, J., Aberson, C., Gurkan, N., Ambrosini, E., Heyman, T., Montefinese, M., vanpaemel, W., & Barzykowski, K. (in press). Accuracy in parameter estimation and simulation approaches for sample size planning with multiple stimuli. Behavioral Research Methods.\n            \n\n            \n            \n                \n                    \n                            accuracy in parameter estimation,\n                        \n                    \n                    \n                            power\n                        \n                    \n                    \n                            sampling\n                        \n                    \n                    \n                            simulation\n                        \n                    \n                    \n                            hypothesis testing\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Prystauka, Y., Colby, S. E., & Drouin, J. R. (in press). Language without borders: A step-by-step guide to analyzing webcam eye-tracking data for L2 research.  Research Methods in Applied Lingustics.https://doi.org/10.1016/j.rmal.2025.100226.\n            \n\n            \n            \n                \n                    \n                            L2 processing\n                        \n                    \n                    \n                            Webcam eye-tracking\n                        \n                    \n                    \n                            R\n                        \n                    \n                    \n                            Spoken word recognition\n                        \n                    \n                    \n                            VWP\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Paper\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Github\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Buchanan, E. M., Cuccolo, K., Coles, N. A...Geller, J.(in press).  Measuring the semantic priming effect across many languages.  Nature Human Behavior .https://doi.org/10.31219/osf.io/q4fjy\n            \n\n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Coretta, S., Casillas, J. V., Roessig, S., Geller, J., et al. (2023). Multidimensional signals and analytic flexibility: Estimating degrees of freedom in human-speech analyses. Advances in Methods and Practices in Psychological Science , 6(3). https://doi.org/10.1177/25152459231162567\n            \n\n            \n            \n                \n                    \n                            crowdsourcing science\n                        \n                    \n                    \n                            data analysis\n                        \n                    \n                    \n                            scientific transparency\n                        \n                    \n                    \n                            speech\n                        \n                    \n                    \n                            acoustic analysis\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Chen-Sankey, J., Elhabashy, M., Gratale, S., Geller, J., Mercincavage, M., Strasser, A. A., Delnevo, C. D., Jeong, M., & Wackowski, O. A. (2023). Examining Visual Attention to Tobacco Marketing Materials Among Young Adult Smokers: Protocol for a Remote Webcam-Based Eye-Tracking Experiment. JMIR research Protocols, 12, e43512. https://doi.org/10.2196/43512\n            \n\n            \n            \n                \n                    \n                            eye tracking\n                        \n                    \n                    \n                            remote eye tracking\n                        \n                    \n                    \n                            e-cigarette marketing\n                        \n                    \n                    \n                            young adults\n                        \n                    \n                    \n                            mobile phone\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Seidman, A. J., Wade, N. G., & Geller, J. (2022). The effects of group counseling and self-affirmation on stigma and group relationship development: A replication and extension. Journal of Counseling Psychology, 69(5), 701‚Äì710. https://doi.org/10.1037/cou0000614\n            \n\n            \n            \n                \n                    \n                            Group counseling\n                        \n                    \n                    \n                            Stigma\n                        \n                    \n                    \n                            Self-affirmation\n                        \n                    \n                    \n                            Intervention\n                        \n                    \n                    \n                            Personal values\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Holmes, A., Schwalje, A., Berger, J. I., Gander, P. E., Choi, I., & McMurray, B. (2021). Validation of the Iowa test of consonant perception. The Journal of the Acoustical Society of America, 150(3), 2131. https://doi.org/10.1121/10.0006246\n            \n\n            \n            \n                \n                    \n                            Auditory percepton\n                        \n                    \n                    \n                            Speech-in-noise\n                        \n                    \n                    \n                            closed-set\n                        \n                    \n                    \n                            Open-set\n                        \n                    \n                    \n                            Validation\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Thye, M., Geller, J., Szaflarski, J. P., & Mirman, D. (2021). Intracranial EEG evidence of functional specialization for taxonomic and thematic relations. Cortex, 140, 40‚Äì50. https://doi.org/10.1016/j.cortex.2021.03.018\n            \n\n            \n            \n                \n                    \n                            Intracranial EEG\n                        \n                    \n                    \n                            Semantic cognition\n                        \n                    \n                    \n                            Taxonomic\n                        \n                    \n                    \n                            Thematic\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., & Peterson, D. (2021). Is this going to be on the test? Test expectancy moderates the disfluency effect with sans forgetica. Journal of Experimental Psychology: Learning, Memory, and Cognition , 47(12), 1924‚Äì1938. https://doi.org/10.1037/xlm0001042\n            \n\n            \n            \n                \n                    \n                            Learning and Memory\n                        \n                    \n                    \n                            Disfluency\n                        \n                    \n                    \n                            Sans Forgetica\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preregistrations\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                McGarrigle, R., Knight, S., Rakusen, L., Geller, J., & Mattys, S. (2021). Older adults show a more sustained pattern of effortful listening than young adults. Psychology and aging, 36(4), 504‚Äì519. https://doi.org/10.1037/pag0000587\n            \n\n            \n            \n                \n                    \n                            Pupil\n                        \n                    \n                    \n                            Effort\n                        \n                    \n                    \n                            Speech\n                        \n                    \n                    \n                            Age\n                        \n                    \n                    \n                            Fatigue\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Davis, S. D., & Peterson, D. J. (2020). Sans Forgetica is not desirable for learning. Memory, 28(8), 957‚Äì967. https://doi.org/10.1080/09658211.2020.1797096\n            \n\n            \n            \n                \n                    \n                            Learning and Memory\n                        \n                    \n                    \n                            Disfluency\n                        \n                    \n                    \n                            Fonts\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preregistration 1\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preregistration 2\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Winn, M. B., Mahr, T., & Mirman, D. (2020). GazeR: A package for processing gaze position and pupil size data. Behavior Research Methods, 52(5), 2232‚Äì2255. https://doi.org/10.3758/s13428-020-01374-8\n            \n\n            \n            \n                \n                    \n                            Pupil\n                        \n                    \n                    \n                            R\n                        \n                    \n                    \n                            Tutorial\n                        \n                    \n                    \n                            Eye-tracking\n                        \n                    \n                    \n                            Preprocessing\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             gazeR\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             CodeOcean\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Carpenter, S. K., & Geller, J. (2020). Is a picture really worth a thousand words? Evaluating contributions of fluency and analytic processing in metacognitive judgements for pictures in foreign language vocabulary learning. Quarterly journal of experimental psychology, 73(2), 211‚Äì224. https://doi.org/10.1177/1747021819879416\n            \n\n            \n            \n                \n                    \n                            Overconfidence\n                        \n                    \n                    \n                            Metacognition\n                        \n                    \n                    \n                            Second language\n                        \n                    \n                    \n                            Fluency\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Landrigan, J. F., & Mirman, D. (2019). A Pupillometric examination of cognitive control in taxonomic and thematic semantic memory. Journal of Cognition, 2(1), 6. https://doi.org/10.5334/joc.56\n            \n\n            \n            \n                \n                    \n                            Pupil\n                        \n                    \n                    \n                            Semantics\n                        \n                    \n                    \n                            Word Recognition\n                        \n                    \n                    \n                            Taxonomic\n                        \n                    \n                    \n                            Thematic\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF Preregistration\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             CodeOcean\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Thye, M., & Mirman, D. (2019). Estimating effects of graded white matter damage and binary tract disconnection on post-stroke language impairment. Neuroimage, 189, 248‚Äì257. https://doi.org/10.1016/j.neuroimage.2019.01.020\n            \n\n            \n            \n                \n                    \n                            Aphasia\n                        \n                    \n                    \n                            Disconnection\n                        \n                    \n                    \n                            Lesion load\n                        \n                    \n                    \n                            Lesion-symptom mapping\n                        \n                    \n                    \n                            Replication\n                        \n                    \n                    \n                            White matter\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             OSF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., Still, M. L., Dark, V. J., & Carpenter, S. K. (2018). Would disfluency by any other name still be disfluent? Examining the disfluency effect with cursive handwriting. Memory & Cognition, 46(7), 1109‚Äì1126. https://doi.org/10.3758/s13421-018-0824-6\n            \n\n            \n            \n                \n                    \n                            Learning and Memory\n                        \n                    \n                    \n                            Disfluency\n                        \n                    \n                    \n                            Metacognition\n                        \n                    \n                    \n                            Cursive\n                        \n                    \n                    \n                            Word Recognition\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Geller, J., & Still, M. L. (2018).Testing expectancy, but not JOLs, moderate the disfluency effect. Proceedings of the 38th Annual Meeting of the Cognitive Science Society. (pp. 1175-1179). Madison, WI: Cognitive Science Society.\n            \n\n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Toftness, A. R., Carpenter, S. K., Geller, J., Lauber, S., Johnson, M., & Armstrong, P. I. (2018). Instructor fluency leads to higher confidence in learning, but not better learning. Metacognition and Learning, 13(1), 1‚Äì14. https://doi.org/10.1007/s11409-017-9175-0\n            \n\n            \n            \n                \n                    \n                            Learning and Memory\n                        \n                    \n                    \n                            Fluencuy\n                        \n                    \n                    \n                            Disfluency\n                        \n                    \n                    \n                            Metacognition\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                 Geller, J., Toftness, A. R., Armstrong, P. I., Carpenter, S. K., Manz, C. L., Coffman, C. R., & Lamm, M. H. (2018). Study strategies and beliefs about learning as a function of academic achievement and achievement goals. Memory, 26(5), 683‚Äì690. https://doi.org/10.1080/09658211.2017.1397175\n            \n\n            \n            \n                \n                    \n                            Education\n                        \n                    \n                    \n                            Learning and Memory\n                        \n                    \n                    \n                            Study Habits\n                        \n                    \n                    \n                            Motivation\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                 Geller, J., Carpenter, S. K., Lamm, M. H., Rahman, S., Armstrong, P. I., & Coffman, C. R. (2017). Prequestions do not enhance the benefits of retrieval in a STEM classroom. Cognitive research: principles and implications, 2(1), 42. https://doi.org/10.1186/s41235-017-0078-z\n            \n\n            \n            \n                \n                    \n                            Prequestions\n                        \n                    \n                    \n                            Retreival practice\n                        \n                    \n                    \n                            Classroom\n                        \n                    \n                    \n                            Engineering education\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                 Geller, J., Still, M. L., & Morris, A. L. (2016). Eyes wide open: Pupil size as a proxy for inhibition in the masked-priming paradigm.Memory & Cognition, 44(4), 554‚Äì564. https://doi.org/10.3758/s13421-015-0577-4\n            \n\n            \n            \n                \n                    \n                            Pupillometry\n                        \n                    \n                    \n                            Word Recognition\n                        \n                    \n                    \n                            Language\n                        \n                    \n                    \n                            Eye-tracking\n                        \n                    \n                    \n                            Inhibition\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller, J., Giner-Sorolla, R., Grange, J. A., Perugini, M., Spies, J. R., & van't Veer, A. (2014). The replication recipe: What makes for a convincing replication? Journal of Experimental Social Psychology, 50, 217‚Äì224. https://doi.org/10.1016/j.jesp.2013.10.005\n            \n\n            \n            \n                \n                    \n                            Replication\n                        \n                    \n                    \n                            Open Science\n                        \n                    \n                    \n                            Psychology\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "Research/articles/2024-05-06-SPAML/index.html",
    "href": "Research/articles/2024-05-06-SPAML/index.html",
    "title": "Measuring the semantic priming effect across many languages",
    "section": "",
    "text": "Semantic priming has been studied for nearly 50 years across various experimental manipulations and theoretical frameworks. These studies provide insight into the cognitive underpinnings of semantic representations in both healthy and clinical populations; however, they have suffered from several issues including generally low sample sizes and a lack of diversity in linguistic implementations. Here, we will test the size and the variability of the semantic priming effect across ten languages by creating a large database of semantic priming values, based on an adaptive sampling procedure. Differences in response latencies between related word-pair conditions and unrelated word-pair conditions (i.e., difference score confidence interval is greater than zero) will allow quantifying evidence for semantic priming, whereas improvements in model fit with the addition of a random intercept for language will provide support for variability in semantic priming across languages."
  },
  {
    "objectID": "Research/articles/2024-05-06-SPAML/index.html#abstract",
    "href": "Research/articles/2024-05-06-SPAML/index.html#abstract",
    "title": "Measuring the semantic priming effect across many languages",
    "section": "",
    "text": "Semantic priming has been studied for nearly 50 years across various experimental manipulations and theoretical frameworks. These studies provide insight into the cognitive underpinnings of semantic representations in both healthy and clinical populations; however, they have suffered from several issues including generally low sample sizes and a lack of diversity in linguistic implementations. Here, we will test the size and the variability of the semantic priming effect across ten languages by creating a large database of semantic priming values, based on an adaptive sampling procedure. Differences in response latencies between related word-pair conditions and unrelated word-pair conditions (i.e., difference score confidence interval is greater than zero) will allow quantifying evidence for semantic priming, whereas improvements in model fit with the addition of a random intercept for language will provide support for variability in semantic priming across languages."
  },
  {
    "objectID": "Research/articles/2020-07-10-pupil-age/index.html",
    "href": "Research/articles/2020-07-10-pupil-age/index.html",
    "title": "Pupillometry reveals a more sustained pattern of effortful listening in older adults",
    "section": "",
    "text": "Listening to speech in adverse conditions can be challenging and effortful, especially for older adults. This study examined age-related differences in effortful listening by recording changes in the task-evoked pupil response (TEPR; a physiological marker of listening effort) both at the level of sentence processing and over the entire course of a listening task. A total of 65 (32 young adults, 33 older adults) participants performed a speech recognition task in the presence of a competing talker, while moment-to-moment changes in pupil size were continuously monitored. Participants were also administered the Vanderbilt Fatigue Scale, a questionnaire assessing daily life listening-related fatigue within four domains (social, cognitive, emotional, physical). Normalized TEPRs were overall larger and more steeply rising and falling around the peak in the older versus the young adult group during sentence processing. Additionally, mean TEPRs over the course of the listening task were more stable in the older versus the young adult group, consistent with a more sustained recruitment of compensatory attentional resources to maintain task performance. No age-related differences were found in terms of total daily life listening-related fatigue; however, older adults reported higher scores than young adults within the social domain. Overall, this study provides evidence for qualitatively distinct patterns of physiological arousal between young and older adults consistent with age-related upregulation in resource allocation during listening. A more detailed understanding of age-related changes in the subjective and physiological mechanisms that underlie effortful listening will ultimately help to address complex communication needs in aging listeners."
  },
  {
    "objectID": "Research/articles/2020-07-10-pupil-age/index.html#abstract",
    "href": "Research/articles/2020-07-10-pupil-age/index.html#abstract",
    "title": "Pupillometry reveals a more sustained pattern of effortful listening in older adults",
    "section": "",
    "text": "Listening to speech in adverse conditions can be challenging and effortful, especially for older adults. This study examined age-related differences in effortful listening by recording changes in the task-evoked pupil response (TEPR; a physiological marker of listening effort) both at the level of sentence processing and over the entire course of a listening task. A total of 65 (32 young adults, 33 older adults) participants performed a speech recognition task in the presence of a competing talker, while moment-to-moment changes in pupil size were continuously monitored. Participants were also administered the Vanderbilt Fatigue Scale, a questionnaire assessing daily life listening-related fatigue within four domains (social, cognitive, emotional, physical). Normalized TEPRs were overall larger and more steeply rising and falling around the peak in the older versus the young adult group during sentence processing. Additionally, mean TEPRs over the course of the listening task were more stable in the older versus the young adult group, consistent with a more sustained recruitment of compensatory attentional resources to maintain task performance. No age-related differences were found in terms of total daily life listening-related fatigue; however, older adults reported higher scores than young adults within the social domain. Overall, this study provides evidence for qualitatively distinct patterns of physiological arousal between young and older adults consistent with age-related upregulation in resource allocation during listening. A more detailed understanding of age-related changes in the subjective and physiological mechanisms that underlie effortful listening will ultimately help to address complex communication needs in aging listeners."
  },
  {
    "objectID": "Research/articles/2021-03-31-testing-SF/index.html",
    "href": "Research/articles/2021-03-31-testing-SF/index.html",
    "title": "Is this going to be on the test? Test expectancy moderates the disfluency effect with Sans Forgetica",
    "section": "",
    "text": "Presenting information in a perceptually disfluent format sometimes enhances memory. Recent work examining 1 type of perceptual disfluency manipulation, Sans Forgetica typeface, has yielded discrepant findings; some studies find support for the idea that the disfluent typeface improves memory whereas others do not. The current study examined a boundary condition that determines when disfluency is and is not beneficial to learning to explore this discrepancy. Specifically, we investigated whether knowledge about an upcoming test (high test expectancy) versus not (low test expectancy) helps clarify when mnemonic benefits arise for perceptually disfluent stimuli. In Experiment 1 (preregistered, N = 231), we found that Sans Forgetica is a memory-improving desirable difficulty, but only when there was no expectation of a final test. In Experiment 2 (preregistered, N = 232), we conceptually replicated the Sans Forgetica effect using a cued-recall test. In Experiment 3 (preregistered, N = 232), we ruled out a time-on-task explanation while replicating the results of Experiment 2. Though these data provide some evidence of San Forgetica‚Äôs mnemonic benefits, caution should be taken in interpreting these results. Not only were the effect sizes moderate, but low test expectancy may not be realistically achievable in actual educational contexts. Though more research is warranted, we echo our prior arguments that students wanting to remember more and forget less should stick to other, more empirically supported desirable difficulties."
  },
  {
    "objectID": "Research/articles/2021-03-31-testing-SF/index.html#abstract",
    "href": "Research/articles/2021-03-31-testing-SF/index.html#abstract",
    "title": "Is this going to be on the test? Test expectancy moderates the disfluency effect with Sans Forgetica",
    "section": "",
    "text": "Presenting information in a perceptually disfluent format sometimes enhances memory. Recent work examining 1 type of perceptual disfluency manipulation, Sans Forgetica typeface, has yielded discrepant findings; some studies find support for the idea that the disfluent typeface improves memory whereas others do not. The current study examined a boundary condition that determines when disfluency is and is not beneficial to learning to explore this discrepancy. Specifically, we investigated whether knowledge about an upcoming test (high test expectancy) versus not (low test expectancy) helps clarify when mnemonic benefits arise for perceptually disfluent stimuli. In Experiment 1 (preregistered, N = 231), we found that Sans Forgetica is a memory-improving desirable difficulty, but only when there was no expectation of a final test. In Experiment 2 (preregistered, N = 232), we conceptually replicated the Sans Forgetica effect using a cued-recall test. In Experiment 3 (preregistered, N = 232), we ruled out a time-on-task explanation while replicating the results of Experiment 2. Though these data provide some evidence of San Forgetica‚Äôs mnemonic benefits, caution should be taken in interpreting these results. Not only were the effect sizes moderate, but low test expectancy may not be realistically achievable in actual educational contexts. Though more research is warranted, we echo our prior arguments that students wanting to remember more and forget less should stick to other, more empirically supported desirable difficulties."
  },
  {
    "objectID": "Research/articles/2020-01-28-gazer/index.html",
    "href": "Research/articles/2020-01-28-gazer/index.html",
    "title": "GazeR: A package for processing gaze postion and pupil size data",
    "section": "",
    "text": "Eye-tracking is widely used throughout the scientific community, from vision science and psycholinguistics to marketing and human-computer interaction. Surprisingly, there is little consistency and transparency in preprocessing steps, making replicability and reproducibility difficult. To increase replicability, reproducibility, and transparency, a package in R (a free and widely used statistical programming environment) called gazeR was created to read and preprocess two types of data: gaze position and pupil size. For gaze position data, gazeR has functions for reading in raw eye-tracking data, formatting it for analysis, converting from gaze coordinates to areas of interest, and binning and aggregating data. For data from pupillometry studies, the gazeR package has functions for reading in and merging multiple raw pupil data files, removing observations with too much missing data, eliminating artifacts, blink identification and interpolation, subtractive baseline correction, and binning and aggregating data. The package is open-source and freely available for download and installation: https://github.com/dmirman/gazer. We provide step-by-step analyses of data from two tasks exemplifying the package‚Äôs capabilities."
  },
  {
    "objectID": "Research/articles/2020-01-28-gazer/index.html#abstract",
    "href": "Research/articles/2020-01-28-gazer/index.html#abstract",
    "title": "GazeR: A package for processing gaze postion and pupil size data",
    "section": "",
    "text": "Eye-tracking is widely used throughout the scientific community, from vision science and psycholinguistics to marketing and human-computer interaction. Surprisingly, there is little consistency and transparency in preprocessing steps, making replicability and reproducibility difficult. To increase replicability, reproducibility, and transparency, a package in R (a free and widely used statistical programming environment) called gazeR was created to read and preprocess two types of data: gaze position and pupil size. For gaze position data, gazeR has functions for reading in raw eye-tracking data, formatting it for analysis, converting from gaze coordinates to areas of interest, and binning and aggregating data. For data from pupillometry studies, the gazeR package has functions for reading in and merging multiple raw pupil data files, removing observations with too much missing data, eliminating artifacts, blink identification and interpolation, subtractive baseline correction, and binning and aggregating data. The package is open-source and freely available for download and installation: https://github.com/dmirman/gazer. We provide step-by-step analyses of data from two tasks exemplifying the package‚Äôs capabilities."
  },
  {
    "objectID": "Research/articles/Eyetracking_smoking/index.html",
    "href": "Research/articles/Eyetracking_smoking/index.html",
    "title": "Protocol for a remote webcam-based eye-tracking experiment for examining visual attention to tobacco marketing materials",
    "section": "",
    "text": "Eye tracking provides an objective way to measure attention, which can advance researchers‚Äô and policy makers‚Äô understanding of tobacco marketing influences. The development of remote webcam-based eye-tracking technology, integrated with web-based crowdsourcing studies, may be a cost-effective and time-efficient alternative to laboratory-based eye-tracking methods. However, research is needed to evaluate the utility of remote eye-tracking methods.\n\n\n\nThis study aimed to detail the process of designing a remote webcam-based eye-tracking experiment and provide data on associations between participant characteristics and the outcomes of experiment completion.\n\n\n\nA total of 2023 young adult (aged 18-34 years) cigarette smokers in the United States were recruited to complete a web-based survey that included a 90-second remote eye-tracking experiment that examined attention to e-cigarette marketing materials. Primary outcome measures assessed the completion of the remote eye-tracking experiment‚Äîspecifically, experiment initiated versus not initiated, experiment completed versus not completed, and usable versus nonusable eye-tracking data generated. Multivariable logistic regressions examined the associations between outcome measures and participants‚Äô sociodemographic backgrounds, tobacco use history, and electronic devices (mobile vs desktop) used during the experiment.\n\n\n\nStudy recruitment began on April 14, 2022, and ended on May 3, 2022. Of the 2023 survey participants, 1887 (93.28%) initiated the experiment, and 777 (38.41%) completed the experiment. Of the 777 participants who completed the experiment, 381 (49%) generated usable data. Results from the full regression models show that non-Hispanic Black participants (adjusted odds ratio [AOR] 0.64, 95% CI 0.45-0.91) were less likely to complete the eye-tracking experiment than non-Hispanic White participants. In addition, female (vs male) participants (AOR 1.46, 95% CI 1.01-2.11), those currently using (vs not using) e-cigarettes (AOR 2.08, 95% CI 1.13-3.82), and those who used mobile (vs desktop) devices (AOR 5.10, 95% CI 3.05-8.52) were more likely to generate usable eye-tracking data.\n\n\n\nYoung adult participants were willing to try remote eye-tracking technology, and nearly half of those who completed the experiment generated usable eye-tracking data (381/777, 49%). Thus, we believe that the use of remote eye-tracking tools, integrated with crowdsourcing recruitment, can be a useful approach for the tobacco regulatory science research community to collect high-quality, large-scale eye-tracking data in a timely fashion and thereby address research questions related to the ever-evolving"
  },
  {
    "objectID": "Research/articles/Eyetracking_smoking/index.html#abstract",
    "href": "Research/articles/Eyetracking_smoking/index.html#abstract",
    "title": "Protocol for a remote webcam-based eye-tracking experiment for examining visual attention to tobacco marketing materials",
    "section": "",
    "text": "Eye tracking provides an objective way to measure attention, which can advance researchers‚Äô and policy makers‚Äô understanding of tobacco marketing influences. The development of remote webcam-based eye-tracking technology, integrated with web-based crowdsourcing studies, may be a cost-effective and time-efficient alternative to laboratory-based eye-tracking methods. However, research is needed to evaluate the utility of remote eye-tracking methods.\n\n\n\nThis study aimed to detail the process of designing a remote webcam-based eye-tracking experiment and provide data on associations between participant characteristics and the outcomes of experiment completion.\n\n\n\nA total of 2023 young adult (aged 18-34 years) cigarette smokers in the United States were recruited to complete a web-based survey that included a 90-second remote eye-tracking experiment that examined attention to e-cigarette marketing materials. Primary outcome measures assessed the completion of the remote eye-tracking experiment‚Äîspecifically, experiment initiated versus not initiated, experiment completed versus not completed, and usable versus nonusable eye-tracking data generated. Multivariable logistic regressions examined the associations between outcome measures and participants‚Äô sociodemographic backgrounds, tobacco use history, and electronic devices (mobile vs desktop) used during the experiment.\n\n\n\nStudy recruitment began on April 14, 2022, and ended on May 3, 2022. Of the 2023 survey participants, 1887 (93.28%) initiated the experiment, and 777 (38.41%) completed the experiment. Of the 777 participants who completed the experiment, 381 (49%) generated usable data. Results from the full regression models show that non-Hispanic Black participants (adjusted odds ratio [AOR] 0.64, 95% CI 0.45-0.91) were less likely to complete the eye-tracking experiment than non-Hispanic White participants. In addition, female (vs male) participants (AOR 1.46, 95% CI 1.01-2.11), those currently using (vs not using) e-cigarettes (AOR 2.08, 95% CI 1.13-3.82), and those who used mobile (vs desktop) devices (AOR 5.10, 95% CI 3.05-8.52) were more likely to generate usable eye-tracking data.\n\n\n\nYoung adult participants were willing to try remote eye-tracking technology, and nearly half of those who completed the experiment generated usable eye-tracking data (381/777, 49%). Thus, we believe that the use of remote eye-tracking tools, integrated with crowdsourcing recruitment, can be a useful approach for the tobacco regulatory science research community to collect high-quality, large-scale eye-tracking data in a timely fashion and thereby address research questions related to the ever-evolving"
  },
  {
    "objectID": "Research/articles/2025-02-17-L2-Webcam/index.html",
    "href": "Research/articles/2025-02-17-L2-Webcam/index.html",
    "title": "Language without borders: A step-by-step guide to analyzing webcam eye-tracking data for L2 research",
    "section": "",
    "text": "Abstract\nEye-tracking has become a valuable tool for studying cognitive processes in second language (L2) acquisition and bilingualism (Godfroid et al., 2024). While research-grade infrared eye-trackers are commonly used, there are a number of issues that limit its wide-spread adoption. Recently, consumer-based webcam eye-tracking has emerged as an attractive alternative, requiring only internet access and a personal webcam. However, webcam eye-tracking presents unique design and preprocessing challenges that must be addressed for valid results. To help researchers overcome these challenges, we developed a comprehensive tutorial focused on visual world webcam eye-tracking for L2 language research. Our guide will cover all key steps,from design to data preprocessing and analysis, where we highlight the R package webgazeR, which is open source and freely available for download and installation:https://github.com/jgeller112/webgazeR. We offer best practices for environmental conditions, participant instructions, and tips for designing visual world experiments with webcam eye-tracking. To demonstrate these steps, we analyze data collected through the Gorilla platform (Anwyl-Irvine et al., 2020) using a single word Span-ish visual world paradigm (VWP) and show competition within and between L2/L1.This tutorial aims to empower researchers by providing a step-by-step guide to successfully conduct visual world webcam-based eye-tracking studies. To follow along with this tutorial, please download the entire manuscript and its accompanying code with data from here: https://github.com/jgeller112/L2_VWP_Webcam"
  },
  {
    "objectID": "Research/articles/2017-11-09-study-beliefs/index.html",
    "href": "Research/articles/2017-11-09-study-beliefs/index.html",
    "title": "Study strategies and beliefs about learning as a function of academic achievement and achievement goals",
    "section": "",
    "text": "Prior research by Hartwig and Dunlosky [(2012). Study strategies of college students: Are self-testing and scheduling related to achievement? Psychonomic Bulletin & Review, 19(1), 126‚Äì134] has demonstrated that beliefs about learning and study strategies endorsed by students are related to academic achievement: higher performing students tend to choose more effective study strategies and are more aware of the benefits of self-testing. We examined whether students‚Äô achievement goals, independent of academic achievement, predicted beliefs about learning and endorsement of study strategies. We administered Hartwig and Dunlosky‚Äôs survey, along with the Achievement Goals Questionnaire [Elliot, A. J., & McGregor, H. A. (2001). A 2 √ó 2 achievement goal framework. Journal of Personality & Social Psychology, 80, 501‚Äì519] to a large undergraduate biology course. Similar to results by Hartwig and Dunlosky, we found that high-performing students (relative to low-performing students) were more likely to endorse self-testing, less likely to cram, and more likely to plan a study schedule ahead of time. Independent of achievement, however, achievement goals were stronger predictors of certain study behaviours. In particular, avoidance goals (e.g., fear of failure) coincided with increased use of cramming and the tendency to be driven by impending deadlines. Results suggest that individual differences in student achievement, as well as the underlying reasons for achievement, are important predictors of students‚Äô approaches to studying."
  },
  {
    "objectID": "Research/articles/2017-11-09-study-beliefs/index.html#abstract",
    "href": "Research/articles/2017-11-09-study-beliefs/index.html#abstract",
    "title": "Study strategies and beliefs about learning as a function of academic achievement and achievement goals",
    "section": "",
    "text": "Prior research by Hartwig and Dunlosky [(2012). Study strategies of college students: Are self-testing and scheduling related to achievement? Psychonomic Bulletin & Review, 19(1), 126‚Äì134] has demonstrated that beliefs about learning and study strategies endorsed by students are related to academic achievement: higher performing students tend to choose more effective study strategies and are more aware of the benefits of self-testing. We examined whether students‚Äô achievement goals, independent of academic achievement, predicted beliefs about learning and endorsement of study strategies. We administered Hartwig and Dunlosky‚Äôs survey, along with the Achievement Goals Questionnaire [Elliot, A. J., & McGregor, H. A. (2001). A 2 √ó 2 achievement goal framework. Journal of Personality & Social Psychology, 80, 501‚Äì519] to a large undergraduate biology course. Similar to results by Hartwig and Dunlosky, we found that high-performing students (relative to low-performing students) were more likely to endorse self-testing, less likely to cram, and more likely to plan a study schedule ahead of time. Independent of achievement, however, achievement goals were stronger predictors of certain study behaviours. In particular, avoidance goals (e.g., fear of failure) coincided with increased use of cramming and the tendency to be driven by impending deadlines. Results suggest that individual differences in student achievement, as well as the underlying reasons for achievement, are important predictors of students‚Äô approaches to studying."
  },
  {
    "objectID": "Research/articles/2013-10-23-replication/index.html",
    "href": "Research/articles/2013-10-23-replication/index.html",
    "title": "The replication recipe: What makes for a convincing replication?",
    "section": "",
    "text": "Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits."
  },
  {
    "objectID": "Research/articles/2013-10-23-replication/index.html#abstract",
    "href": "Research/articles/2013-10-23-replication/index.html#abstract",
    "title": "The replication recipe: What makes for a convincing replication?",
    "section": "",
    "text": "Psychological scientists have recently started to reconsider the importance of close replications in building a cumulative knowledge base; however, there is no consensus about what constitutes a convincing close replication study. To facilitate convincing close replication attempts we have developed a Replication Recipe, outlining standard criteria for a convincing close replication. Our Replication Recipe can be used by researchers, teachers, and students to conduct meaningful replication studies and integrate replications into their scholarly habits."
  },
  {
    "objectID": "Research/articles/2023-07-20-speechperception-opensci/index.html",
    "href": "Research/articles/2023-07-20-speechperception-opensci/index.html",
    "title": "Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses",
    "section": "",
    "text": "Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis that can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling but also from decisions regarding the quantification of the measured behavior. In this study, we gave the same speech-production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further found little to no evidence that the observed variability can be explained by analysts‚Äô prior beliefs, expertise, or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system, and calibrate their (un)certainty in their conclusions."
  },
  {
    "objectID": "Research/articles/2023-07-20-speechperception-opensci/index.html#abstract",
    "href": "Research/articles/2023-07-20-speechperception-opensci/index.html#abstract",
    "title": "Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses",
    "section": "",
    "text": "Recent empirical studies have highlighted the large degree of analytic flexibility in data analysis that can lead to substantially different conclusions based on the same data set. Thus, researchers have expressed their concerns that these researcher degrees of freedom might facilitate bias and can lead to claims that do not stand the test of time. Even greater flexibility is to be expected in fields in which the primary data lend themselves to a variety of possible operationalizations. The multidimensional, temporally extended nature of speech constitutes an ideal testing ground for assessing the variability in analytic approaches, which derives not only from aspects of statistical modeling but also from decisions regarding the quantification of the measured behavior. In this study, we gave the same speech-production data set to 46 teams of researchers and asked them to answer the same research question, resulting in substantial variability in reported effect sizes and their interpretation. Using Bayesian meta-analytic tools, we further found little to no evidence that the observed variability can be explained by analysts‚Äô prior beliefs, expertise, or the perceived quality of their analyses. In light of this idiosyncratic variability, we recommend that researchers more transparently share details of their analysis, strengthen the link between theoretical construct and quantitative system, and calibrate their (un)certainty in their conclusions."
  },
  {
    "objectID": "Research/articles/2021-11-02-ITCP/index.html",
    "href": "Research/articles/2021-11-02-ITCP/index.html",
    "title": "Validation of the Iowa test of consonant perception",
    "section": "",
    "text": "Speech perception (especially in background noise) is a critical problem for hearing-impaired listeners and an important issue for cognitive hearing science. Despite a plethora of standardized measures, few single-word closed-set tests uniformly sample the most frequently used phonemes and use response choices that equally sample phonetic features like place and voicing. The Iowa Test of Consonant Perception (ITCP) attempts to solve this. It is a proportionally balanced phonemic word recognition task designed to assess perception of the initial consonant of monosyllabic consonant-vowel-consonant (CVC) words. The ITCP consists of 120 sampled CVC words. Words were recorded from four different talkers (two female) and uniformly sampled from all four quadrants of the vowel space to control for coarticulation. Response choices on each trial are balanced to equate difficulty and sample a single phonetic feature. This study evaluated the psychometric properties of ITCP by examining reliability (test-retest) and validity in a sample of online normal-hearing participants. Ninety-eight participants completed two sessions of the ITCP along with standardized tests of words and sentence in noise (CNC words and AzBio sentences). The ITCP showed good test-retest reliability and convergent validity with two popular tests presented in noise. All the materials to use the ITCP or to construct your own version of the ITCP are freely available [Geller, McMurray, Holmes, and Choi (2020). https://osf.io/hycdu/]."
  },
  {
    "objectID": "Research/articles/2021-11-02-ITCP/index.html#abstract",
    "href": "Research/articles/2021-11-02-ITCP/index.html#abstract",
    "title": "Validation of the Iowa test of consonant perception",
    "section": "",
    "text": "Speech perception (especially in background noise) is a critical problem for hearing-impaired listeners and an important issue for cognitive hearing science. Despite a plethora of standardized measures, few single-word closed-set tests uniformly sample the most frequently used phonemes and use response choices that equally sample phonetic features like place and voicing. The Iowa Test of Consonant Perception (ITCP) attempts to solve this. It is a proportionally balanced phonemic word recognition task designed to assess perception of the initial consonant of monosyllabic consonant-vowel-consonant (CVC) words. The ITCP consists of 120 sampled CVC words. Words were recorded from four different talkers (two female) and uniformly sampled from all four quadrants of the vowel space to control for coarticulation. Response choices on each trial are balanced to equate difficulty and sample a single phonetic feature. This study evaluated the psychometric properties of ITCP by examining reliability (test-retest) and validity in a sample of online normal-hearing participants. Ninety-eight participants completed two sessions of the ITCP along with standardized tests of words and sentence in noise (CNC words and AzBio sentences). The ITCP showed good test-retest reliability and convergent validity with two popular tests presented in noise. All the materials to use the ITCP or to construct your own version of the ITCP are freely available [Geller, McMurray, Holmes, and Choi (2020). https://osf.io/hycdu/]."
  },
  {
    "objectID": "Research/articles/2022-03-28-counseling-stigma/index.html",
    "href": "Research/articles/2022-03-28-counseling-stigma/index.html",
    "title": "The Effects of Group Counseling and Self-Affirmation on Stigma and Group Relationship Development: A Replication and Extension",
    "section": "",
    "text": "The stigma of seeking counseling and negative attitudes about counseling are primary barriers to its use. In the only known study examining the utility of attending a group counseling session to ameliorate stigma (no control group), participation was associated with reductions in self-stigma (Wade et al., 2011). Self-affirmation interventions have shown promising results in reducing stigma and promoting positive expectations about counseling, but no research has examined its effects on a counseling session. In the present, two-part study, 172 college students who had previously completed an online screening survey, including measures of stigma, participated in a single session of group counseling at a mental health clinic. Upon arrival, participants completed a self-affirmation intervention before viewing psychoeducation (n = 66; 12 groups) or only viewed psychoeducation (n = 72; 14 groups); both groups then completed a session of group counseling. After, participants completed these same measures along with measures of group relationships. The remaining participants (n = 34; 7 groups) viewed psychoeducation and completed the same stigma measures before being informed of randomization to the waitlist-control condition. Our results replicate and extend findings from Wade et al.¬†(2011): completing a single session of group counseling reduced self-stigma and promoted positive attitudes toward counseling. Further, completing self-affirmation reduced post-session perceptions of public stigma. Self-affirmation had no impact on group relationships. Overall, findings suggest the utility of offering a ‚Äútry-out‚Äù session of group counseling as a stigma-reduction intervention; preceding with a brief self-affirmation intervention provides further benefits by reducing perceptions of public stigma."
  },
  {
    "objectID": "Research/articles/2022-03-28-counseling-stigma/index.html#abstract",
    "href": "Research/articles/2022-03-28-counseling-stigma/index.html#abstract",
    "title": "The Effects of Group Counseling and Self-Affirmation on Stigma and Group Relationship Development: A Replication and Extension",
    "section": "",
    "text": "The stigma of seeking counseling and negative attitudes about counseling are primary barriers to its use. In the only known study examining the utility of attending a group counseling session to ameliorate stigma (no control group), participation was associated with reductions in self-stigma (Wade et al., 2011). Self-affirmation interventions have shown promising results in reducing stigma and promoting positive expectations about counseling, but no research has examined its effects on a counseling session. In the present, two-part study, 172 college students who had previously completed an online screening survey, including measures of stigma, participated in a single session of group counseling at a mental health clinic. Upon arrival, participants completed a self-affirmation intervention before viewing psychoeducation (n = 66; 12 groups) or only viewed psychoeducation (n = 72; 14 groups); both groups then completed a session of group counseling. After, participants completed these same measures along with measures of group relationships. The remaining participants (n = 34; 7 groups) viewed psychoeducation and completed the same stigma measures before being informed of randomization to the waitlist-control condition. Our results replicate and extend findings from Wade et al.¬†(2011): completing a single session of group counseling reduced self-stigma and promoted positive attitudes toward counseling. Further, completing self-affirmation reduced post-session perceptions of public stigma. Self-affirmation had no impact on group relationships. Overall, findings suggest the utility of offering a ‚Äútry-out‚Äù session of group counseling as a stigma-reduction intervention; preceding with a brief self-affirmation intervention provides further benefits by reducing perceptions of public stigma."
  },
  {
    "objectID": "Research/articles/2018-11-02-disflu-handwriting/index.html",
    "href": "Research/articles/2018-11-02-disflu-handwriting/index.html",
    "title": "Would disfluency by any other name still be disfluent? Examining the disfluency effect with cursive handwriting",
    "section": "",
    "text": "When exposed to words presented under perceptually disfluent conditions (e.g., words written in Haettenschweiler font), participants have difficulty initially recognizing the words. Those same words, though, may be better remembered later than words presented in standard type font. This counterintuitive finding is referred to as the disfluency effect. Evidence for this disfluency effect, however, has been mixed, suggesting possible moderating factors. Using a recognition memory task, level of disfluency was examined as a moderating factor across three experiments using a novel cursive manipulation that varied on degree of legibility (easy-to-read cursive vs.¬†hard-to-read cursive). In addition, list type and retention interval between study and test were manipulated. Across all three experiments, cursive words engendered better memory than type-print words. This memory effect persisted across varied list designs (blocked vs.¬†mixed) and a longer (24-hour) retention interval. A small-scale meta-analysis across the three experiments suggested that the cursive disfluency effect is moderated by level of disfluency: easy-to-read cursive words tended to be better remembered than hard-to-read cursive words. Taken together, these results challenge extant accounts of the disfluency effect. The theoretical and practical implications of these findings are discussed."
  },
  {
    "objectID": "Research/articles/2018-11-02-disflu-handwriting/index.html#abstract",
    "href": "Research/articles/2018-11-02-disflu-handwriting/index.html#abstract",
    "title": "Would disfluency by any other name still be disfluent? Examining the disfluency effect with cursive handwriting",
    "section": "",
    "text": "When exposed to words presented under perceptually disfluent conditions (e.g., words written in Haettenschweiler font), participants have difficulty initially recognizing the words. Those same words, though, may be better remembered later than words presented in standard type font. This counterintuitive finding is referred to as the disfluency effect. Evidence for this disfluency effect, however, has been mixed, suggesting possible moderating factors. Using a recognition memory task, level of disfluency was examined as a moderating factor across three experiments using a novel cursive manipulation that varied on degree of legibility (easy-to-read cursive vs.¬†hard-to-read cursive). In addition, list type and retention interval between study and test were manipulated. Across all three experiments, cursive words engendered better memory than type-print words. This memory effect persisted across varied list designs (blocked vs.¬†mixed) and a longer (24-hour) retention interval. A small-scale meta-analysis across the three experiments suggested that the cursive disfluency effect is moderated by level of disfluency: easy-to-read cursive words tended to be better remembered than hard-to-read cursive words. Taken together, these results challenge extant accounts of the disfluency effect. The theoretical and practical implications of these findings are discussed."
  },
  {
    "objectID": "Research/articles/2018-07-31-testexpt-masking/index.html",
    "href": "Research/articles/2018-07-31-testexpt-masking/index.html",
    "title": "Testing expectancy, but not judgements of learning, moderate the disfluency effect",
    "section": "",
    "text": "Do students learn better with material that is perceptually harder-to-process? Previous research has been equivocal concerning this question. To clarify these discrepancies, the present study examined two potential boundary conditions to determine when disfluent text is, and is not, beneficial to learning. The two boundary conditions examined were: type of judgement of learning (JOLs) and testing expectancy. Boundary conditions were examined in separate Group (incidental aggregate JOLs vs.¬†intentional aggregate JOLs vs.¬†item-by-item JOLs) by Disfluency (Masked vs.¬†Nonmasked) mixed ANOVAs. Results revealed that type of JOL did not moderate the disfluency effect, but testing expectancy did. These results bring forth questions pertaining to the utility of disfluency on learning."
  },
  {
    "objectID": "Research/articles/2018-07-31-testexpt-masking/index.html#abstract",
    "href": "Research/articles/2018-07-31-testexpt-masking/index.html#abstract",
    "title": "Testing expectancy, but not judgements of learning, moderate the disfluency effect",
    "section": "",
    "text": "Do students learn better with material that is perceptually harder-to-process? Previous research has been equivocal concerning this question. To clarify these discrepancies, the present study examined two potential boundary conditions to determine when disfluent text is, and is not, beneficial to learning. The two boundary conditions examined were: type of judgement of learning (JOLs) and testing expectancy. Boundary conditions were examined in separate Group (incidental aggregate JOLs vs.¬†intentional aggregate JOLs vs.¬†item-by-item JOLs) by Disfluency (Masked vs.¬†Nonmasked) mixed ANOVAs. Results revealed that type of JOL did not moderate the disfluency effect, but testing expectancy did. These results bring forth questions pertaining to the utility of disfluency on learning."
  },
  {
    "objectID": "Research/articles/2018-04-29-instrut-flu/index.html",
    "href": "Research/articles/2018-04-29-instrut-flu/index.html",
    "title": "Instructor fluency leads to higher confidence in learning, but not better learning",
    "section": "",
    "text": "Students‚Äô judgements of their own learning often exceed their knowledge on a given topic. One source of this pervasive overconfidence is fluency, the perceived ease with which information is acquired. Though effects of fluency on metacognitive judgments have been explored by manipulating relatively simple stimuli such as font style, few studies have explored the effects of fluency on more complex forms of learning encountered in educational settings, such as learning from lectures. The present study manipulated the fluency of a 31-min video-recorded lecture, and measured its effects on both perceived and actual learning. In the fluent condition, the instructor used non-verbal gestures, voice dynamics, mobility about the space, and appropriate pauses. In the disfluent condition, the same instructor read directly from notes, hunched over a podium, rarely made eye contact, used few non-verbal gestures, spoke in monotone pitch, and took irregular and awkward pauses. Though participants rated the fluent instructor significantly higher than the disfluent instructor on measures of teaching effectiveness and estimated that they had learned more of the material, actual learning between the two groups did not differ as assessed by a memory test over the lecture contents given immediately (Experiment 1) or after a 1-day delay (Experiment 2). This counterintuitive result reveals an ‚Äúillusion of learning‚Äù due to fluency in lecture-based learning, a very common form of instruction."
  },
  {
    "objectID": "Research/articles/2018-04-29-instrut-flu/index.html#abstract",
    "href": "Research/articles/2018-04-29-instrut-flu/index.html#abstract",
    "title": "Instructor fluency leads to higher confidence in learning, but not better learning",
    "section": "",
    "text": "Students‚Äô judgements of their own learning often exceed their knowledge on a given topic. One source of this pervasive overconfidence is fluency, the perceived ease with which information is acquired. Though effects of fluency on metacognitive judgments have been explored by manipulating relatively simple stimuli such as font style, few studies have explored the effects of fluency on more complex forms of learning encountered in educational settings, such as learning from lectures. The present study manipulated the fluency of a 31-min video-recorded lecture, and measured its effects on both perceived and actual learning. In the fluent condition, the instructor used non-verbal gestures, voice dynamics, mobility about the space, and appropriate pauses. In the disfluent condition, the same instructor read directly from notes, hunched over a podium, rarely made eye contact, used few non-verbal gestures, spoke in monotone pitch, and took irregular and awkward pauses. Though participants rated the fluent instructor significantly higher than the disfluent instructor on measures of teaching effectiveness and estimated that they had learned more of the material, actual learning between the two groups did not differ as assessed by a memory test over the lecture contents given immediately (Experiment 1) or after a 1-day delay (Experiment 2). This counterintuitive result reveals an ‚Äúillusion of learning‚Äù due to fluency in lecture-based learning, a very common form of instruction."
  },
  {
    "objectID": "Research/articles/2020-05-28-SF-undesirable/index.html",
    "href": "Research/articles/2020-05-28-SF-undesirable/index.html",
    "title": "Sans Forgetica is not desirable for learning",
    "section": "",
    "text": "Do students learn better with material that is perceptually hard to process? While evidence is mixed, recent claims suggest that placing materials in Sans Forgetica, a perceptually difficult-to-process typeface, has positive impacts on student learning. Given the weak evidence for other similar perceptual disfluency effects, we examined the mnemonic effects of Sans Forgetica more closely in comparison to other learning strategies across three preregistered experiments. In Experiment 1, participants studied weakly related cue-target pairs with targets presented in either Sans Forgetica or with missing letters (e.g., cue: G_RL, the generation effect). Cued recall performance showed a robust effect of generation, but no Sans Forgetica memory benefit. In Experiment 2, participants read an educational passage about ground water with select sentences presented in either Sans Forgetica typeface, yellow pre-highlighting, or unmodified. Cued recall for select words was better for pre-highlighted information than an unmodified pure reading condition. Critically, presenting sentences in Sans Forgetica did not elevate cued recall compared to an unmodified pure reading condition or a pre-highlighted condition. In Experiment 3, individuals did not have better discriminability for Sans Forgetica relative to a fluent condition in an old-new recognition test. Our findings suggest that Sans Forgetica really is forgettable."
  },
  {
    "objectID": "Research/articles/2020-05-28-SF-undesirable/index.html#abstract",
    "href": "Research/articles/2020-05-28-SF-undesirable/index.html#abstract",
    "title": "Sans Forgetica is not desirable for learning",
    "section": "",
    "text": "Do students learn better with material that is perceptually hard to process? While evidence is mixed, recent claims suggest that placing materials in Sans Forgetica, a perceptually difficult-to-process typeface, has positive impacts on student learning. Given the weak evidence for other similar perceptual disfluency effects, we examined the mnemonic effects of Sans Forgetica more closely in comparison to other learning strategies across three preregistered experiments. In Experiment 1, participants studied weakly related cue-target pairs with targets presented in either Sans Forgetica or with missing letters (e.g., cue: G_RL, the generation effect). Cued recall performance showed a robust effect of generation, but no Sans Forgetica memory benefit. In Experiment 2, participants read an educational passage about ground water with select sentences presented in either Sans Forgetica typeface, yellow pre-highlighting, or unmodified. Cued recall for select words was better for pre-highlighted information than an unmodified pure reading condition. Critically, presenting sentences in Sans Forgetica did not elevate cued recall compared to an unmodified pure reading condition or a pre-highlighted condition. In Experiment 3, individuals did not have better discriminability for Sans Forgetica relative to a fluent condition in an old-new recognition test. Our findings suggest that Sans Forgetica really is forgettable."
  },
  {
    "objectID": "Research/articles/2019-01-24-pupil-semantics/index.html",
    "href": "Research/articles/2019-01-24-pupil-semantics/index.html",
    "title": "Is a picture really worth a thousand words? Evaluating contributions of fluency and analytic processing in metacognitive judgements for pictures in foreign language vocabulary learning",
    "section": "",
    "text": "Previous research shows that participants are overconfident in their ability to learn foreign language vocabulary from pictures compared with English translations. The current study explored whether this tendency is due to processing fluency or beliefs about learning. Using self-paced study of Swahili words paired with either picture cues or English translation cues, picture cues garnered higher confidence judgements but not faster study times, and this was true whether judgements of learning were made after a delay (Experiment 1) or immediately (Experiment 2). In Experiment 3, when participants learned Swahili words with only one type of cue (pictures or English translations) and then estimated which one would be more effective for learning, the majority of participants believed pictures would be more effective regardless of whether they had experienced those cues during learning. Experiment 4 showed the same results when participants had experienced neither type of cue during a learning phase. These results suggest that metacognitive judgements in foreign language vocabulary learning are driven more by students‚Äô beliefs about learning than by processing fluency as reflected in self-paced study times."
  },
  {
    "objectID": "Research/articles/2019-01-24-pupil-semantics/index.html#abstract",
    "href": "Research/articles/2019-01-24-pupil-semantics/index.html#abstract",
    "title": "Is a picture really worth a thousand words? Evaluating contributions of fluency and analytic processing in metacognitive judgements for pictures in foreign language vocabulary learning",
    "section": "",
    "text": "Previous research shows that participants are overconfident in their ability to learn foreign language vocabulary from pictures compared with English translations. The current study explored whether this tendency is due to processing fluency or beliefs about learning. Using self-paced study of Swahili words paired with either picture cues or English translation cues, picture cues garnered higher confidence judgements but not faster study times, and this was true whether judgements of learning were made after a delay (Experiment 1) or immediately (Experiment 2). In Experiment 3, when participants learned Swahili words with only one type of cue (pictures or English translations) and then estimated which one would be more effective for learning, the majority of participants believed pictures would be more effective regardless of whether they had experienced those cues during learning. Experiment 4 showed the same results when participants had experienced neither type of cue during a learning phase. These results suggest that metacognitive judgements in foreign language vocabulary learning are driven more by students‚Äô beliefs about learning than by processing fluency as reflected in self-paced study times."
  },
  {
    "objectID": "Research/articles/2019-01-16-whitematter-disconnection/index.html",
    "href": "Research/articles/2019-01-16-whitematter-disconnection/index.html",
    "title": "Estimating effects of graded white matter damage and binary tract disconnection on post-stroke language impairment",
    "section": "",
    "text": "Despite the critical importance of close replications in strengthening and advancing scientific knowledge, there are inherent challenges to conducting replications of lesion-based studies. In the present study, we conducted a close conceptual replication of a study (i.e., Hope et al., 2016) that found that fluency and naming scores in post-stoke aphasia were more strongly associated with a binary measure of structural white matter integrity (tract disconnection) than a graded measure (lesion load). Using a different sample of stroke patients (N = 128) and four language deficit measures (aphasia severity, picture naming, and composite scores for speech production and semantic cognition), we examined tract disconnection and lesion load in three white matter tracts that have been implicated in language processing: arcuate fasciculus, uncinate fasciculus, and inferior fronto-occipital fasciculus. We did not find any consistent evidence that binary tract disconnection was more strongly associated with language impairment over and above lesion load, though individual deficit measures differed with respect to whether lesion load or tract disconnection was the stronger predictor. Given the mixed findings, we suggest caution when using such indirect estimates of structural white matter integrity, and direct individual measurements (for example, using diffusion weighted imaging) should be preferred when they are available. We end by highlighting the complex nature of replication in lesion-based studies and offer some potential solutions."
  },
  {
    "objectID": "Research/articles/2019-01-16-whitematter-disconnection/index.html#abstract",
    "href": "Research/articles/2019-01-16-whitematter-disconnection/index.html#abstract",
    "title": "Estimating effects of graded white matter damage and binary tract disconnection on post-stroke language impairment",
    "section": "",
    "text": "Despite the critical importance of close replications in strengthening and advancing scientific knowledge, there are inherent challenges to conducting replications of lesion-based studies. In the present study, we conducted a close conceptual replication of a study (i.e., Hope et al., 2016) that found that fluency and naming scores in post-stoke aphasia were more strongly associated with a binary measure of structural white matter integrity (tract disconnection) than a graded measure (lesion load). Using a different sample of stroke patients (N = 128) and four language deficit measures (aphasia severity, picture naming, and composite scores for speech production and semantic cognition), we examined tract disconnection and lesion load in three white matter tracts that have been implicated in language processing: arcuate fasciculus, uncinate fasciculus, and inferior fronto-occipital fasciculus. We did not find any consistent evidence that binary tract disconnection was more strongly associated with language impairment over and above lesion load, though individual deficit measures differed with respect to whether lesion load or tract disconnection was the stronger predictor. Given the mixed findings, we suggest caution when using such indirect estimates of structural white matter integrity, and direct individual measurements (for example, using diffusion weighted imaging) should be preferred when they are available. We end by highlighting the complex nature of replication in lesion-based studies and offer some potential solutions."
  },
  {
    "objectID": "Research/articles/2019-10-02-pic-flu/index.html",
    "href": "Research/articles/2019-10-02-pic-flu/index.html",
    "title": "A pupilometric examination of cognitive control in taxonomic and thematic semantic memory",
    "section": "",
    "text": "Semantic cognition includes taxonomic and thematic relationships, as well as control systems to retrieve and manipulate semantic knowledge to suit specific tasks or contexts. A recent report (Thompson et al., 2017) suggested that retrieving thematic relationships (i.e., relations based on participation in the same event or scenarios) requires more effort or cognitive control, especially when the relevant relations are weak, than retrieving identity relations that are based on sensory-motor features. It is not clear whether the same contrast applies to the broader set of taxonomic relations, which are also based on shared sensory-motor features. In this study we tested cognitive control requirements of retrieving taxonomic and thematic knowledge using a physiological measure of cognitive effort: pupil dilation. Participants completed a semantic relatedness judgement task that manipulated semantic type (thematic vs.¬†taxonomic) and relatedness strength (high vs.¬†low) of word pairs. Cognitive control in the similarity task was examined using task-evoked pupillary responses (TEPRs), as well as standard behavioral measures (reaction times and accuracy). Compared with high-strength relations, low-strength semantic relations elicited larger TERPs, slower reaction times, and lower accuracy, consistent with higher control demands. Compared to thematic relations, taxonomic relations also elicited larger TERPs and slower reaction times, suggesting that retrieving taxonomic relations required more cognitive effort. Critically, our pupillometric data indicated that controlled processing was particularly important for low-strength taxonomic pairs rather than low-strength thematic pairs. These findings indicate that semantic control demands are primarily determined by relatedness strength, not whether the relationship is taxonomic or thematic."
  },
  {
    "objectID": "Research/articles/2019-10-02-pic-flu/index.html#abstract",
    "href": "Research/articles/2019-10-02-pic-flu/index.html#abstract",
    "title": "A pupilometric examination of cognitive control in taxonomic and thematic semantic memory",
    "section": "",
    "text": "Semantic cognition includes taxonomic and thematic relationships, as well as control systems to retrieve and manipulate semantic knowledge to suit specific tasks or contexts. A recent report (Thompson et al., 2017) suggested that retrieving thematic relationships (i.e., relations based on participation in the same event or scenarios) requires more effort or cognitive control, especially when the relevant relations are weak, than retrieving identity relations that are based on sensory-motor features. It is not clear whether the same contrast applies to the broader set of taxonomic relations, which are also based on shared sensory-motor features. In this study we tested cognitive control requirements of retrieving taxonomic and thematic knowledge using a physiological measure of cognitive effort: pupil dilation. Participants completed a semantic relatedness judgement task that manipulated semantic type (thematic vs.¬†taxonomic) and relatedness strength (high vs.¬†low) of word pairs. Cognitive control in the similarity task was examined using task-evoked pupillary responses (TEPRs), as well as standard behavioral measures (reaction times and accuracy). Compared with high-strength relations, low-strength semantic relations elicited larger TERPs, slower reaction times, and lower accuracy, consistent with higher control demands. Compared to thematic relations, taxonomic relations also elicited larger TERPs and slower reaction times, suggesting that retrieving taxonomic relations required more cognitive effort. Critically, our pupillometric data indicated that controlled processing was particularly important for low-strength taxonomic pairs rather than low-strength thematic pairs. These findings indicate that semantic control demands are primarily determined by relatedness strength, not whether the relationship is taxonomic or thematic."
  },
  {
    "objectID": "Research/articles/2025-disfluency-modeling/index.html",
    "href": "Research/articles/2025-disfluency-modeling/index.html",
    "title": "A Response Time Distributional Analysis of the Perceptual Disfluency Effect",
    "section": "",
    "text": "Abstract\nPerceptual disfluency, induced by blurring or difficult-to-read typefaces, can sometimes enhance memory retention, but the underlying mechanisms remain unclear. To investigate this effect, we manipulated blurring levels (clear, low blur, high blur) during encoding and assessed recognition performance in a surprise memory test. In Experiments 1A and 1B, response latencies from a lexical decision task were analyzed using ex-Gaussian distribution modeling and drift diffusion modeling. Results showed that blurring differentially influenced parameters of the model, with high blur affecting both early and late-stage processes, while low blur primarily influenced early-stage processes. Recognition test results further revealed that high blur words were remembered better than both clear and low blurred words. Experiment 2 employed a semantic categorization task with word frequency manipulation to further examine the locus of the perceptual disfluency effect. Similar to Experiments 1A and 1B, high blur influenced both early and late-stage processes, while low blur primarily affected early-stage processes. Low-frequency words exhibited greater shifting and skewing in distributional parameters, yet only high-frequency, highly blurred words demonstrated an enhanced memory effect. These findings suggest that both early and late cognitive processes contribute to the mnemonic benefits associated with perceptual disfluency. Overall, this study demonstrates that distributional and computational analyses provide powerful tools for dissecting encoding mechanisms and their effects on memory, offering valuable insights into models of perceptual disfluency."
  },
  {
    "objectID": "Research/articles/2016-05-20-inhib-pupil-neighbor/index.html",
    "href": "Research/articles/2016-05-20-inhib-pupil-neighbor/index.html",
    "title": "Eyes wide open: Pupil size as a proxy for inhibition in the masked-priming paradigm",
    "section": "",
    "text": "A core assumption underlying competitive-network models of word recognition is that in order for a word to be recognized, the representations of competing orthographically similar words must be inhibited. This inhibitory mechanism is revealed in the masked-priming lexical-decision task (LDT) when responses to orthographically similar word prime‚Äìtarget pairs are slower than orthographically different word prime‚Äìtarget pairs (i.e., inhibitory priming). In English, however, behavioral evidence for inhibitory priming has been mixed. In the present study, we utilized a physiological correlate of cognitive effort never before used in the masked-priming LDT, pupil size, to replicate and extend behavioral demonstrations of inhibitory effects (i.e., Nakayama, Sears, & Lupker, Journal of Experimental Psychology: Human Perception and Performance, 34, 1236‚Äì1260, 2008, Exp. 1). Previous research had suggested that pupil size is a reliable indicator of cognitive load, making it a promising index of lexical inhibition. Our pupillometric data replicated and extended previous behavioral findings, in that inhibition was obtained for orthographically similar word prime‚Äìtarget pairs. However, our response time data provided only a partial replication of Nakayama et al.¬†Journal of Experimental Psychology: Human Perception and Performance, 34, 1236‚Äì1260, 2008. These results provide converging lines of evidence that inhibition operates in word recognition and that pupillometry is a useful addition to word recognition researchers‚Äô toolbox."
  },
  {
    "objectID": "Research/articles/2016-05-20-inhib-pupil-neighbor/index.html#abstract",
    "href": "Research/articles/2016-05-20-inhib-pupil-neighbor/index.html#abstract",
    "title": "Eyes wide open: Pupil size as a proxy for inhibition in the masked-priming paradigm",
    "section": "",
    "text": "A core assumption underlying competitive-network models of word recognition is that in order for a word to be recognized, the representations of competing orthographically similar words must be inhibited. This inhibitory mechanism is revealed in the masked-priming lexical-decision task (LDT) when responses to orthographically similar word prime‚Äìtarget pairs are slower than orthographically different word prime‚Äìtarget pairs (i.e., inhibitory priming). In English, however, behavioral evidence for inhibitory priming has been mixed. In the present study, we utilized a physiological correlate of cognitive effort never before used in the masked-priming LDT, pupil size, to replicate and extend behavioral demonstrations of inhibitory effects (i.e., Nakayama, Sears, & Lupker, Journal of Experimental Psychology: Human Perception and Performance, 34, 1236‚Äì1260, 2008, Exp. 1). Previous research had suggested that pupil size is a reliable indicator of cognitive load, making it a promising index of lexical inhibition. Our pupillometric data replicated and extended previous behavioral findings, in that inhibition was obtained for orthographically similar word prime‚Äìtarget pairs. However, our response time data provided only a partial replication of Nakayama et al.¬†Journal of Experimental Psychology: Human Perception and Performance, 34, 1236‚Äì1260, 2008. These results provide converging lines of evidence that inhibition operates in word recognition and that pupillometry is a useful addition to word recognition researchers‚Äô toolbox."
  },
  {
    "objectID": "Research/articles/2021-04-07-sEEG/index.html",
    "href": "Research/articles/2021-04-07-sEEG/index.html",
    "title": "Intracranial EEG evidence of functional specialization for taxonomic and thematic relations",
    "section": "",
    "text": "The dual-hub account posits that the neural organization of semantic knowledge is segregated by the type of semantic relation with anterior temporal lobe (ATL) specializing for taxonomic relations and inferior parietal lobule (IPL) for thematic relations. This study critically examined this account by recording intracranial EEG from an array of depth electrodes within ATL, IPL, and two regions within the semantic control network, inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG), while 17 participants with refractory epilepsy completed a semantic relatedness judgment task. We observed a significant difference between relation types in ATL and IPL approximately 600-800 ms after trial presentation, and no significant differences in IFG or pMTG. Within this time window, alpha and theta suppression indexing cognitive effort and memory retrieval was observed in ATL for taxonomic trials and in IPL for thematic trials. These results suggest taxonomic specialization in ATL and thematic specialization in IPL, consistent with the dual-hub account of semantic cognition."
  },
  {
    "objectID": "Research/articles/2021-04-07-sEEG/index.html#abstract",
    "href": "Research/articles/2021-04-07-sEEG/index.html#abstract",
    "title": "Intracranial EEG evidence of functional specialization for taxonomic and thematic relations",
    "section": "",
    "text": "The dual-hub account posits that the neural organization of semantic knowledge is segregated by the type of semantic relation with anterior temporal lobe (ATL) specializing for taxonomic relations and inferior parietal lobule (IPL) for thematic relations. This study critically examined this account by recording intracranial EEG from an array of depth electrodes within ATL, IPL, and two regions within the semantic control network, inferior frontal gyrus (IFG) and posterior middle temporal gyrus (pMTG), while 17 participants with refractory epilepsy completed a semantic relatedness judgment task. We observed a significant difference between relation types in ATL and IPL approximately 600-800 ms after trial presentation, and no significant differences in IFG or pMTG. Within this time window, alpha and theta suppression indexing cognitive effort and memory retrieval was observed in ATL for taxonomic trials and in IPL for thematic trials. These results suggest taxonomic specialization in ATL and thematic specialization in IPL, consistent with the dual-hub account of semantic cognition."
  },
  {
    "objectID": "Research/preprints/2025-coding-brain/index.html",
    "href": "Research/preprints/2025-coding-brain/index.html",
    "title": "The coding brain: P600 effects elicited by the visual programming language ScratchJr",
    "section": "",
    "text": "Abstract\nIn recent years, there is a growing emphasis on early childhood coding education to equip students with essential skills for a growingly technical society. While emerging research has investigated how text-based programming languages are processed in the adult brain, the neural mechanisms underlying early childhood visual programming languages like ScratchJr remain largely unexplored. This preregistered study examined the cognitive basis of ScratchJr processing, with two primary research aims: 1. Investigate whether ScratchJr elicits neural signatures comparable to natural languages and other rule-based processes (specifically N400 and semantic P600 responses) and 2. Explore how experience level with ScratchJr modulates these effects. We recorded event-related potentials (ERPs) from 40 (N = 40) adults while they completed a semantic congruency task using ScratchJr stimuli. Neural responses were analyzed in time bins associated with N400 and P600 components, and experience level with ScratchJr was included as a potential factor to modulate these results. Results revealed a P600 response to semantic violations, with no modulation by experience level. These findings contribute to growing evidence of the semantic nature of the P600 and support the notion that ScratchJr processing evokes neural mechanisms related to structured processes such as natural language."
  },
  {
    "objectID": "Talks/index.html",
    "href": "Talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page contains the source code, links, and slides for various workshops, talks, lectures, and presentations I‚Äôve given."
  },
  {
    "objectID": "Talks/index.html#section",
    "href": "Talks/index.html#section",
    "title": "Talks",
    "section": "2025",
    "text": "2025\n\nWorkshop\n\n\n\n    \n    \n                  \n            August 8, 2025\n        \n        \n            Writing Reproducible Manuscripts in R using Quarto\n            Virtual talk given for OPAM community. Open Science practices are transforming how research is conducted, emphasizing transparency, reproducibility, and collaboration. In this hands-on workshop, you‚Äôll learn how to build a fully reproducible workflow for writing and publishing a scientific manuscript that integrates text, data, and code. | OPAM 2025 Virtual\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Companion site and materials\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Video\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            April 3, 2025\n        \n        \n            Reproducible Manuscripts in R with Quarto Workshop\n            Open Science practices‚Äîcentered on transparency, reproducibility, and accessibility‚Äîare becoming increasingly essential in psychology. In this 2-hour workshop, you‚Äôll learn how to set up a fully reproducible workflow to generate a publication-ready manuscript that seamlessly integrates data, R or Python code, text, and references. Special emphasis will be placed on formatting APA-style manuscripts. Additionally, I‚Äôll introduce Nix/Rix as a solution for managing package and system dependencies to ensure long-term reproducibility. | Boston College\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Website\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Video\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Talks/index.html#section-1",
    "href": "Talks/index.html#section-1",
    "title": "Talks",
    "section": "2021",
    "text": "2021\n\n\n\n    \n    \n                  \n            June 29, 2021\n        \n        \n            What Did You Say? A Web-Based Validation of a Speech-In-Noise Task\n            BeOnline is the conference to learn all about online behavioral research. It‚Äôs the ideal place to discover the challenges and benefits of online research and to learn from pioneers. | Online (Zoom)\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n                \n                    \n                    \n                    \n                         GitHub repository for talk\n                    \n                \n                \n                \n                    \n                    \n                    \n                         Video\n                    \n                \n                \n            \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Talks/index.html#section-2",
    "href": "Talks/index.html#section-2",
    "title": "Talks",
    "section": "2020",
    "text": "2020"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Thanks for stopping by!\nI am a researcher, educator, and lover of R (Python is pretty cool too) and stats. I study how we read words and understand speech (especially in noise). I am also interested in how we can get students to remember more and forget less.\nPlease send review requests and correspondence to jason.geller[at]bc.edu"
  },
  {
    "objectID": "Software/packages/webgazer.html",
    "href": "Software/packages/webgazer.html",
    "title": "webgazeR",
    "section": "",
    "text": "üêô GitHub: jgeller112/webgazeR\nüíª Website: Package site\nüìÑ Documentation: Read the Paper\n\n\n\nCitationBibTeX citation:@online{geller,\n  author = {Geller, Jason},\n  title = {webgazeR},\n  url = {jgeller112.github.io/webgazeR/},\n  doi = {https://doi.org/10.5281/zenodo.15831767},\n  langid = {en},\n  abstract = {webgazeR is a webcam eye-tracking toolkit for\n    pre-processing webcam gaze data}\n}\nFor attribution, please cite this work as:\nGeller, J. (n.d.). webgazeR. https://doi.org/https://doi.org/10.5281/zenodo.15831767"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "",
    "text": "In this vignette I am going to show you how to read in a GazePoint data file along with some behavioral data and use gazeR to preprocess the data.\nSpecial thanks to Matthew K Robinson (Twitter:@matthewkrobinson) for letting me use some data from an auditory oddball task he conducted on himself (we do what we have to do as researchers :D): see Tweet below.\nTo get started, we need to load in some important packages and read in the GP data files."
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#load-packages",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#load-packages",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(remotes)\nremotes::install_github(\"dmirman/gazer\")\nlibrary(gazer)\nlibrary(data.table)\nlibrary(here)\nremotes::install_github(\"tmalsburg/saccades/saccades\", dependencies=TRUE)\nlibrary(saccades)"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#read-data",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#read-data",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Read Data",
    "text": "Read Data\n\npd &lt;- fread('oddball_eye_13.tsv') # eye data \nbs&lt;-fread('oddball_13.tsv') # behave data\n\nhead(pd)\n\n     CNT     TIME    TIME_TICK    FPOGX     FPOGY    FPOGS   FPOGD FPOGID FPOGV\n   &lt;int&gt;    &lt;num&gt;        &lt;i64&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;   &lt;num&gt;  &lt;int&gt; &lt;int&gt;\n1: 77433 1251.964 102150661899 -4.40518 -13.98604 1251.883 0.08093   1917     1\n2: 77434 1251.980 102150821625 -4.39508 -13.95200 1251.883 0.09692   1917     1\n3: 77435 1251.996 102150983307 -4.36427 -13.84678 1251.883 0.09692   1917     0\n4: 77436 1252.013 102151149297 -4.40694 -13.99395 1251.883 0.09692   1917     0\n5: 77437 1252.028 102151307171 -4.42096 -14.04383 1251.883 0.09692   1917     0\n6: 77438 1252.045 102151469190 -4.40730 -14.00445 1251.883 0.09692   1917     0\n     LPOGX   LPOGY LPOGV    RPOGX     RPOGY RPOGV    BPOGX     BPOGY BPOGV\n     &lt;num&gt;   &lt;num&gt; &lt;int&gt;    &lt;num&gt;     &lt;num&gt; &lt;int&gt;    &lt;num&gt;     &lt;num&gt; &lt;int&gt;\n1: 0.71795 0.50537     1 -9.38697 -28.00086     1 -4.33451 -13.74775     1\n2: 0.71795 0.50537     1 -9.38697 -28.00086     1 -4.33451 -13.74775     1\n3: 0.70711 0.51136     1 -9.14516 -27.18135     1 -4.21902 -13.33500     1\n4: 0.70151 0.43055     1 -9.60071 -28.71280     1 -4.44960 -14.14113     1\n5: 0.70270 0.42562     1 -9.60071 -28.71280     1 -4.44900 -14.14359     1\n6: 0.69827 0.43625     1 -9.34484 -27.89350     1 -4.32329 -13.72862     1\n      LPCX    LPCY      LPD     LPS   LPV    RPCX    RPCY      RPD   RPS   RPV\n     &lt;num&gt;   &lt;num&gt;    &lt;num&gt;   &lt;num&gt; &lt;int&gt;   &lt;num&gt;   &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;int&gt;\n1: 0.34860 0.59214 25.23420 0.83829     1 0.64310 0.60511 28.49240     1     1\n2: 0.34860 0.59216 25.27127 0.83829     1 0.64314 0.60509 28.33148     1     1\n3: 0.34845 0.59234 25.09805 0.83829     1 0.64293 0.60517 28.69580     1     1\n4: 0.34846 0.59237 25.15947 0.83829     1 0.64286 0.60519 28.47081     1     1\n5: 0.34838 0.59240 25.32249 0.84484     1 0.64285 0.60518 28.46784     1     1\n6: 0.34830 0.59240 25.07680 0.85139     1 0.64269 0.60527 28.50588     1     1\n      LEYEX    LEYEY   LEYEZ LPUPILD LPUPILV   REYEX    REYEY   REYEZ RPUPILD\n      &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;int&gt;   &lt;num&gt;    &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n1: -0.03362 -0.01726 0.56538 0.00455       1 0.03052 -0.01989 0.57411 0.00512\n2: -0.03438 -0.01769 0.57821 0.00451       1 0.03056 -0.01997 0.57644 0.00513\n3: -0.03438 -0.01769 0.57821 0.00449       1 0.03056 -0.01997 0.57644 0.00510\n4: -0.03438 -0.01769 0.57821 0.00451       1 0.03056 -0.01997 0.57644 0.00509\n5: -0.03501 -0.01795 0.58649 0.00448       1 0.03130 -0.02049 0.59132 0.00509\n6: -0.03501 -0.01795 0.58649 0.00450       1 0.03130 -0.02049 0.59132 0.00508\n   RPUPILV      CX      CY    CS  BKID BKDUR BKPMIN    LPMM LPMMV    RPMM RPMMV\n     &lt;int&gt;   &lt;num&gt;   &lt;num&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;  &lt;int&gt;   &lt;num&gt; &lt;int&gt;   &lt;num&gt; &lt;int&gt;\n1:       1 0.33333 0.33333     0     0     0     20 4.54567     1 5.11614     1\n2:       1 0.33333 0.33333     0     0     0     20 4.51065     1 5.12683     1\n3:       1 0.33333 0.33333     0     0     0     20 4.48607     1 5.10490     1\n4:       1 0.33333 0.33333     0     0     0     20 4.50748     1 5.09460     1\n5:       1 0.33333 0.33333     0     0     0     20 4.47957     1 5.08827     1\n6:       1 0.33333 0.33333     0     0     0     20 4.50258     1 5.07552     1\n    DIAL DIALV   GSR  GSRV    HR   HRV   HRP  TTL0  TTL1  TTLV            USER\n   &lt;num&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;          &lt;char&gt;\n1: 0.088     1     0     0     0     0   454    -1    -1     0               0\n2: 0.088     1     0     0     0     0   484    -1    -1     0 STARTEXPERIMENT\n3: 0.088     1     0     0     0     0   456    -1    -1     0 STARTEXPERIMENT\n4: 0.088     1     0     0     0     0   451    -1    -1     0           START\n5: 0.088     1     0     0     0     0   482    -1    -1     0           START\n6: 0.088     1     0     0     0     0   454    -1    -1     0           START\n\nhead(bs)\n\n   subject trial   tone    rt response\n     &lt;int&gt; &lt;int&gt; &lt;char&gt; &lt;int&gt;   &lt;char&gt;\n1:      13     1     lo  2113     None\n2:      13     2     lo  2102     None\n3:      13     3     lo  2107     None\n4:      13     4     lo  2108     None\n5:      13     5     lo  2107     None\n6:      13     6     lo  2103     None\n\n\nWhat we are going to do is run the GazePoint file through the merge_gazepoint function. The function below takes a list of files called file_list and merges all the files together, appends a subject column, creates a trial column using the USER column (GazePoint only allows messages through this channel), creates a time variable (in milliseconds). In the merge_gazepoint function the trail_msg argument requires users to denote a message used in the USER column that references the start of the trial‚Äìin our case the START message denotes the start of a new trial. This is a solution by Matt Robinson, but there are other ways one could extract the trial number. What I have done in the past is append a message with the trial iteration (e.g., START_1) in Python and use the separate function to get the trial number.\n\n# A \"monocular mean\" averages both eyes together. If data is available in just\n# one eye, use the available value as the mean, unless we need_both is TRUE.\n#' @param x1 pupil left\n#' @param x2 pupil right\n#' @return vector with monocular mean values\ncompute_monocular_mean &lt;- function(x1, x2) {\n  xm &lt;- rowMeans(cbind(x1, x2), na.rm = TRUE)\n  # NaN =&gt; NA\n  ifelse(is.nan(xm), NA, xm)\n}\n\n\n# function for processing GazePoint data\nmerge_gazepoint &lt;- function (file_list, trial_msg = \"START\"){\n  #file list is path to .xls files\n  #vroom is faster\n  library(data.table)\n  \n  file_ids=str_replace_all(basename(file_list),\"([:alpha:]|[:punct:])\",\"\") # remove everything but numeric values\n                   \n  data &lt;- map2(file_list, file_ids, ~fread(.x) %&gt;% \n    mutate(id = .y))  %&gt;% \n    bind_rows()\n\n  \n  d = data %&gt;%\n    dplyr::rowwise() %&gt;%\n    dplyr::mutate(pupil=compute_monocular_mean(RPMM, LPMM)) %&gt;% # average both eyes\n             dplyr::ungroup() %&gt;%\n           dplyr::mutate(pupil = ifelse(RPMMV == 0|LPMMV == 0, 0, pupil),  #missing data labeled as blinks\n         new_trial = ifelse(USER == trial_msg & lag(USER) != trial_msg, 1, 0), # Label new trials\n         trial = cumsum(new_trial), # Create a trial variable\n         time = floor(TIME*1000)) %&gt;%\n    group_by(trial) %&gt;%\n    dplyr::mutate(time=time - min(time)) %&gt;%\n    ungroup() %&gt;%\n  dplyr::select(id, time,trial,pupil,BPOGX, BPOGY, USER) %&gt;%\n    dplyr::rename(\"message\" = \"USER\", \"subject\"= \"id\", \"x\" = \"BPOGX\", \"y\" = \"BPOGY\") %&gt;% \n    dplyr::filter(trial &gt; 0)\n  \n  return(d)\n}"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#merge-files",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#merge-files",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Merge Files",
    "text": "Merge Files\n\nsetwd(here()) # setwd\n\ngp_file&lt;-list.files(here::here(), pattern = \"eye_13.tsv\") # get files \n\nsetwd(here())\n      \nd=merge_gazepoint(gp_file, trial_msg = \"START\")\n\nd$subject&lt;-as.numeric(d$subject)\n\npdb &lt;- full_join(bs, d)\n\npdb &lt;- as_tibble(pdb)\n\npdb\n\n# A tibble: 73,724 √ó 10\n   subject trial tone     rt response  time pupil     x     y message\n     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1      13     1 lo     2113 None         0  4.80 -4.45 -14.1 START  \n 2      13     1 lo     2113 None        16  4.78 -4.45 -14.1 START  \n 3      13     1 lo     2113 None        32  4.79 -4.32 -13.7 START  \n 4      13     1 lo     2113 None        48  4.80 -4.58 -14.5 START  \n 5      13     1 lo     2113 None        64  4.80 -4.58 -14.5 START  \n 6      13     1 lo     2113 None        81  4.79 -4.63 -14.7 START  \n 7      13     1 lo     2113 None        97  4.81 -4.42 -14.0 START  \n 8      13     1 lo     2113 None       113  4.80 -4.42 -14.0 TONE   \n 9      13     1 lo     2113 None       129  4.78 -4.23 -13.5 TONE   \n10      13     1 lo     2113 None       145  4.77 -4.34 -13.8 TONE   \n# ‚Ñπ 73,714 more rows"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#blinks",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#blinks",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Blinks",
    "text": "Blinks\nFinding Blinks\nThe GazePoint data does not indicate where blinks occurred. What we are going to do is use the blink_detect function in gazer. This relies on the saccades package (https://github.com/tmalsburg/saccades) which uses a velocity based measure based on X,Y coordinates to find blinks. Once we find the blinks we can change the pupil size at that time point as NA and interpolate over it.\nAs a note, the GazePoint does not seem to sample consistently. In this case, it samples every 16 or 17 ms. This is a problem for some other blink detection measures (e.g., the noise based pupil function). One soultion would be to downsample the data at the onset so there is consistancy from sample to sample.\n\nblinks_merge&lt;- blink_detect(pdb)\n\n blinks &lt;- blinks_merge %&gt;%\n        dplyr::group_by(grp = cumsum(!is.na(startend))) %&gt;%\n        dplyr::mutate(Label = replace(startend, first(startend) == 'start', 'start')) %&gt;% #extends the start message forward until end message\n        dplyr::ungroup() %&gt;%\n        # label blinks as 1\n        dplyr::select(subject, trial, time, x, y, pupil, message, tone,  Label, -grp)\n \n \n blinks_data &lt;- blinks  %&gt;%\n        dplyr::mutate(blink=ifelse(!is.na(Label), 1, 0), pupil=ifelse(blink==1 | pupil==0, NA, pupil))%&gt;%\n        dplyr::ungroup()%&gt;%\n        dplyr::select(subject, time, trial, pupil, x, y, trial, message, tone, blink, -Label)\n\nHere is a look at the trials containing blinks:\n\nblinks_data %&gt;% \n  filter(blink==1) %&gt;%\n  head()%&gt;%\n  knitr::kable()\n\n\n\nsubject\ntime\ntrial\npupil\nx\ny\nmessage\ntone\nblink\n\n\n\n13\n2167\n10\nNA\n0.9364\n1.95015\nPOSTTONE\nlo\n1\n\n\n13\n2183\n10\nNA\n0.9364\n1.95015\nPOSTTONE\nlo\n1\n\n\n13\n2199\n10\nNA\n0.9364\n1.95015\nPOSTTONE\nlo\n1\n\n\n13\n2216\n10\nNA\n0.9364\n1.95015\nPOSTTONE\nlo\n1\n\n\n13\n0\n11\nNA\n0.9364\n1.95015\nSTART\nhi\n1\n\n\n13\n16\n11\nNA\n0.9364\n1.95015\nSTART\nhi\n1\n\n\n\n\n\nExtending Blinks\nI am extending blinks 100 ms forward and backward in time.\nInterpolate Blinks\nHere let‚Äôs linearly interpolate the blinks and then smooth the data using a 5-point moving average.\n\n# Smooth and Interpolate\nsmooth_interp &lt;- smooth_interpolate_pupil(pup_extend, pupil=\"pupil\", extendpupil=\"extendpupil\", extendblinks=TRUE, step.first=\"smooth\", maxgap=Inf, type=\"linear\", hz=60, n=5)"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#plot-interpolated-trial",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#plot-interpolated-trial",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Plot Interpolated Trial",
    "text": "Plot Interpolated Trial\n\ninterp_graph &lt;- smooth_interp  %&gt;%\n  dplyr::filter(trial==\"400\")\n\nbold &lt;- element_text(face = \"bold\", color = \"black\", size = 14) #axis bold\n#Graph interpolation\npup_g&lt;- ggplot(interp_graph, aes(x= time, y= pupil)) + geom_point()+ geom_line(colour=\"black\") +\n  geom_line(aes(x=time, y=pup_interp), colour=\"darkgreen\") + xlab(\"Time (ms)\") + ylab(\"Pupil Size (mm)\") + theme_bw() + theme(axis.title.y=element_text(size = 16, face=\"bold\"), axis.title.x = element_text(size=16, face=\"bold\"), axis.text.x=element_text(size = 12, face=\"bold\"), axis.text.y=element_text(size=12, face=\"bold\"))\nprint(pup_g)"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#baseline-correction",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#baseline-correction",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Baseline Correction",
    "text": "Baseline Correction\nHere we will do a subtractive baseline correction taking 250 ms before the onset of the tone as baseline.\n\n#use messages to baseline correct\nbaseline_pupil&lt;-baseline_correction_pupil(smooth_interp, pupil_colname=\"pup_interp\", baseline_window=c(0,250), baseline_method = \"sub\")\n\nhead(baseline_pupil)\n\n# A tibble: 6 √ó 13\n  subject trial baseline  time pupil     x     y message tone  blink extendpupil\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1      13     1     4.80     0  4.80 -4.45 -14.1 START   lo        0        4.80\n2      13     1     4.80    16  4.78 -4.45 -14.1 START   lo        0        4.78\n3      13     1     4.80    32  4.79 -4.32 -13.7 START   lo        0        4.79\n4      13     1     4.80    48  4.80 -4.58 -14.5 START   lo        0        4.80\n5      13     1     4.80    64  4.80 -4.58 -14.5 START   lo        0        4.80\n6      13     1     4.80    81  4.79 -4.63 -14.7 START   lo        0        4.79\n# ‚Ñπ 2 more variables: pup_interp &lt;dbl&gt;, baselinecorrectedp &lt;dbl&gt;"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#missing-data",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#missing-data",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Missing Data",
    "text": "Missing Data\nLet‚Äôs see how much missing data there is and remove trials with greater than 20% missing data.\n\npup_missing&lt;-count_missing_pupil(baseline_pupil, missingthresh = .5)\n# remove outliers\n\nI remove about 15 percent of trials."
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#unlikely-pupil-sizes",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#unlikely-pupil-sizes",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Unlikely Pupil Sizes",
    "text": "Unlikely Pupil Sizes\nNow let‚Äôs keep pupil diameter sizes between 2 mm and 9 mm\n\npup_outliers&lt;-pup_missing %&gt;%\n  dplyr::filter (pup_interp  &gt;= 2, pup_interp &lt;= 9)"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#mad",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#mad",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "MAD",
    "text": "MAD\nGet rid of artifacts we might have missed during some earlier steps.\n\n  #MAD removal\nmax_removal&lt;-pup_missing  %&gt;%\n  dplyr::group_by(subject, trial) %&gt;%\n  dplyr::mutate(speed=speed_pupil(pup_interp,time)) %&gt;%\n  dplyr::mutate(MAD=calc_mad(speed)) %&gt;%\n  dplyr::filter(speed &lt; MAD)"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#onset",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#onset",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Onset",
    "text": "Onset\nLet‚Äôs only look fron the start of the trial until 1500 ms\n\nbaseline_pupil_onset&lt;-max_removal %&gt;%\n  dplyr::group_by(subject, trial) %&gt;%\n  dplyr::filter(time &lt;= 1500) %&gt;%\n  select(subject, trial, time,baselinecorrectedp, tone, time,message,baselinecorrectedp)"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#downsample",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#downsample",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Downsample",
    "text": "Downsample\nDownsample the time-course to 50 ms.\n\n#downsample\ntimebins1&lt;- downsample_gaze(baseline_pupil_onset, bin.length=50, timevar = \"time\", aggvars = c(\"subject\", \"tone\", \"timebins\"), type=\"pupil\")\n\ntimebins1\n\n# A tibble: 62 √ó 4\n   subject tone  timebins aggbaseline\n     &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1      13 hi           0    0.00130 \n 2      13 hi          50   -0.000752\n 3      13 hi         100   -0.00206 \n 4      13 hi         150    0.000331\n 5      13 hi         200    0.00530 \n 6      13 hi         250    0.00503 \n 7      13 hi         300    0.00826 \n 8      13 hi         350    0.00930 \n 9      13 hi         400    0.0116  \n10      13 hi         450    0.0125  \n# ‚Ñπ 52 more rows"
  },
  {
    "objectID": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#visualize-time-course",
    "href": "blog/posts/2021-04-21-gazePoint_gazer_2/index.html#visualize-time-course",
    "title": "Analzying GazePoint Pupil Data With GazeR",
    "section": "Visualize Time-course",
    "text": "Visualize Time-course\n\ncursive_plot &lt;-ggplot(timebins1)+\n  aes(timebins, aggbaseline, linetype=tone, color=tone) +\n  stat_summary(fun = \"mean\", geom = \"line\", size = 1) +\n  theme_bw() +\n  labs(x =\"Time (ms)\",y =\"Pupil Dilation (baseline - pupil))\") +\n  geom_hline(yintercept=0.0)\n\nprint(cursive_plot)\n\n\n\n\n\n\n\nThis looks very similar to the one in the Tweet albeit a bit smoother as a result of the extra preprocessing done."
  },
  {
    "objectID": "blog/posts/2020-07-10-CBPT/index.html",
    "href": "blog/posts/2020-07-10-CBPT/index.html",
    "title": "Mass Univariate Analysis for Pupillometric Data: Cluster Permutation Test",
    "section": "",
    "text": "An article by Reilly et al. (2019) raised some important questions/issues for the field of pupillometry. One issue that has been bothering me as of late (hence the post) pertains to objectively defining a time window to look at. Although some commonalities exist across the literature, there is no uniform standard for isolating a peak range or delineating the width of this window. In this blog post I present a method we can use to address this issue. To do so, I will be using the example data set from the gazeR package (Geller et al. (n.d.)). This data comes from a lexical decison experiment where participants judged the lexicality of cursive and type-print stimuli."
  },
  {
    "objectID": "blog/posts/2020-07-10-CBPT/index.html#common-approaches",
    "href": "blog/posts/2020-07-10-CBPT/index.html#common-approaches",
    "title": "Mass Univariate Analysis for Pupillometric Data: Cluster Permutation Test",
    "section": "Common Approaches",
    "text": "Common Approaches\nIn the pupillometry literature, I have seen several different analytic approaches to the above issue. Some will preform seperate statistical tests at each time point (via t tests, LMMs, ANOVAs, regressions, what have you), or they just ignore time altogether and aggregate the data and perfrom analyses on the mean and max. I do not like these approaches for a few reasons:\n\nThere is a trade-off between power and temporal resolution (smaller time bins) and introducesbias in selection of time bins/windows. There is also the issues of multiple comparisons and autocorrelation\nDoes not take into account individual differences\n\nI wanted to highlight a method I believe is a good alternative: cluster-based permutation tests (henceforth CPT)"
  },
  {
    "objectID": "blog/posts/2020-07-10-CBPT/index.html#cpt",
    "href": "blog/posts/2020-07-10-CBPT/index.html#cpt",
    "title": "Mass Univariate Analysis for Pupillometric Data: Cluster Permutation Test",
    "section": "CPT",
    "text": "CPT\nLet‚Äôs first look at a graph of the data.\n\n\nError in `select()`:\n! Can't select columns that don't exist.\n‚úñ Column `X1` doesn't exist.\n\n\n\n\n\n\n\n\nFrom this, we might be tempted to declare a significant difference arisisng ~1500-2500 ms. However, we cannot rely on the graphical analysis and we need a statistical test to be able to affirm that this observation is caused by diffierntial effort between the two scripts.\nPermuation cluster analysis is a technique that is becoming increasingly popular, especially in the cognitvie neuropsychology domain to analyze MEG-EEG (Maris & Oostenveld, 2007). While there exists a number of cluster analysis functions in MNE-Python (Gramfort et al., 2014) and Matlab‚Äôs FieldTrip (Oostenveld et al., 2011), what I want to show you is how you can do this analysis in R. The implementation of CPT has not been widely used in pupillometry research. I will hopefully show you that it can be a very useful tool to have in your aresenal.\nBefore I show you how to apply this method, I want to briefly go over the CPT method. The CPT is a data-driven method which increases power and controls for Type I errors across multiple comparisons (exatcly what we need when looking at pupil changes across the time course!). The clustering method involves conducting dependent-sample t-tests for every data point (condition by time). In the first step, adjacent data points that cross the mass-univariate significance threshold (p &lt; .05) are combined to create a cluster. The sum of the t-statistic (or F) are calculated and form the basis for the cluster level statistic. In the second step, a surrogate null-distribution is created by first randomly assigning one of the two conditions within subjects (this is done n times) and retaining the cluster statistic for each randomization. In the third and final step, the cluster level statistic of the real comparison is compared against the null distribution, with clusters falling in the highest or lowest 2.5% considered to be significant.\nWhile writing this blog post, I saw that Dale Barr presented on CPT in the context of eye-tracking. I figured I would take this opportunity to test out the packages he created.\n\ndevtools::install_github(\"dalejbarr/exchangr\")\ndevtools::install_github(\"dalejbarr/clusterperm\")\nlibrary(exchangr)\nlibrary(clusterperm)\n\nThe ‚Äòclusterperm‚Äô package has a pretty nifty aov_by_bin function. If we applied a multlipe comparison correction (e.g., holm) to this data.\n\ncur2 &lt;- aov_by_bin(agg_subject, timebins,   # clusterperm package\n  aggbaseline ~ script + Error(subject))\n\ncur2$p_adjuct&lt;-p.adjust(cur2$p, method=\"holm\")\n\ncur2_p=subset(cur2, cur2$p_adjuct &lt;= .05)\n\nknitr::kable(cur2_p)\n\n\n\ntimebins\neffect\nstat\np\np_adjuct\n\n\n\n2000\nscript\n12.97351\n0.0008456\n0.0202952\n\n\n2100\nscript\n17.08613\n0.0001720\n0.0044723\n\n\n2200\nscript\n14.51092\n0.0004587\n0.0114670\n\n\n2300\nscript\n12.96859\n0.0008473\n0.0202952\n\n\n\n\n\nHow does it compare to CPT?\n\norig &lt;- detect_clusters_by_effect(cur2, effect, timebins, stat, p)\n\nknitr::kable(orig)\n\n\n\neffect\nb0\nb1\nsign\ncms\n\n\n\nscript\n0\n100\n1\n10.121215\n\n\nscript\n900\n1000\n-1\n8.756152\n\n\nscript\n1900\n2500\n1\n82.198279\n\n\n\n\n\nIt looks like the multiple comparison correction is less sensitive.\nWait! we do not have p-values!!! Where are the damn p-values!? What is life? How am I suppose to make an informed decison? What is cool about Dale‚Äôs R package is that it has a permutation function that allows you to build the null model and obtain those coveted p-values.\n\ndat_prec &lt;- nest(agg_subject, -subject, -script)\n\nnhds_prec &lt;- cluster_nhds(\n  100, dat_prec, timebins,\n  aggbaseline ~ script + Error(subject),\n  shuffle_each, script, subject) # only use 100 permutations. Should use 1000-2000. \n\nError in `tidyr::nest()`:\n! In expression named `d`:\nCaused by error:\n! Can't select columns that don't exist.\n‚úñ Column `timebins` doesn't exist.\n\n## get p-values\nresults_prec &lt;- pvalues(orig, nhds_prec)\n\nError in eval(expr, envir, enclos): object 'nhds_prec' not found\n\nknitr::kable(results_prec)\n\nError in eval(expr, envir, enclos): object 'results_prec' not found\n\n\nWhew! I feel much better now!\nFrom this graph we can see that there is one signifcant cluster: from 1900-2500 ms. If you were using this as a more explortatory approach (which I think we should), we know have a time period to look at where we can perform more common tests (e.g., max or mean during that time period). We could alternatively make the inference that a difference between cursive and print-type arises somewhere around 1900-2500 ms.\n\n\nError in `select()`:\n! Can't select columns that don't exist.\n‚úñ Column `X1` doesn't exist."
  },
  {
    "objectID": "blog/posts/2020-07-10-CBPT/index.html#thoughts",
    "href": "blog/posts/2020-07-10-CBPT/index.html#thoughts",
    "title": "Mass Univariate Analysis for Pupillometric Data: Cluster Permutation Test",
    "section": "Thoughts",
    "text": "Thoughts\nThere are some misunderstandings users should be made aware of when it comes to CPT (see Sassenhagen & Draschkow, 2019) for a nice discussion). If we want to ask more specific questions (i.e., at what exact time points effects arise), CPT with a cluster mass correction is not the method to use. We can only be certain that a difference exists; we cannot make claims about whether individual time points show an effect with a given error rate. I think this is more of an issue for M/EEG than pupillometry though. The pupillary signal is very slow.\nOverall, I think CPT can be a very useful tool in determing an appropiate time range to look at, and can even lead to more powerful inferences. Further, the use of CPT obviates the need for multiple comparisons and autocorrelation corrections. However, it does have issues. For instance, it cannot take into account subject and item variability. In addition, it is not able to tell you where within the cluster there is a difference only that there is a difference. And finally, when power is low, clusters may include many false postive time points (Fields & Kuperberg, 2019).\nIn my next blog, I will discuss applying generalized additive mixed modeling to your pupillometry data. I think it might be the perfect soultion!"
  },
  {
    "objectID": "Teaching/index.html",
    "href": "Teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Fundamentals of Statistics in Psychological Science \n                \n            \n            \n                PSY503 | \n                Princeton University\n                \n            \n            This graduate-level course covers foundations of statistics in psychological research. It is required of all first-year students in the psychology Ph.D. program. The purpose of this course is to introduce students to statistics with an emphasis on modeling. We cover many of the most widely applied data analysis models in psychology. We focus on data visualization, effect size estimation and interpretation, and using statistical analysis to inform scientific research questions. We develop practical skills related to data management, reproducibility, and statistical programming through the use of R, R Markdown/Quarto, and Github.\n\n            \n                \n                \n                 Fall 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Advances in Statistical Methods and Research Practices in Psychology \n                \n            \n            \n                PSY505 | \n                Princeton University\n                \n            \n            Psychological methods keep evolving: psychology researchers constantly adopt new statistical methods, computational tools, and research practices. This course introduces the latest advances in research methods in psychological science through a series of lectures, tutorials, and seminars. Lectures are generally given by a guest speaker who introduces methodological and statistical issues relevant to psychology research. Students engage in a group discussion with the speaker before the lecture. Tutorials provide students with the necessary skills to build reproducible and transparent research programs. For instance, students learn how to use R, RMarkdown and Git/GitHub, or how to preregister their studies. Finally, students take part in seminars, in which they discuss timely topics in psychological methods (e.g., reproducibility, transparency, pre-registration).\n\n            \n                \n                \n                 Fall 2023\n                \n                \n                \n                 Spring 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Advanced Statistical Medthods \n                \n            \n            \n                PSY504 | \n                Princeton University\n                \n            \n            As a discipline, statistics has given profound tools to the sciences. Statistics is our way of detecting patterns in the noisy universe. Traditionally, psychologists were limited to research questions that could be answered with classical statistical analyses, but without particularly intending to do so, psychologists have found themselves at the cutting age of statistical discovery in the 20th and 21st centuries. This is because we needed to adapt our tools to meet the demands of the complex nature of the phenomena we study. This course will give you the quantitative tools to ask and answer research questions that were impossible to test as recently as a decade ago. Specifically, we will cover the following topics as stand-alone modules\n\n            \n                \n                \n                 Spring 2024\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Teaching/index.html#section",
    "href": "Teaching/index.html#section",
    "title": "Teaching",
    "section": "",
    "text": "Fundamentals of Statistics in Psychological Science \n                \n            \n            \n                PSY503 | \n                Princeton University\n                \n            \n            This graduate-level course covers foundations of statistics in psychological research. It is required of all first-year students in the psychology Ph.D. program. The purpose of this course is to introduce students to statistics with an emphasis on modeling. We cover many of the most widely applied data analysis models in psychology. We focus on data visualization, effect size estimation and interpretation, and using statistical analysis to inform scientific research questions. We develop practical skills related to data management, reproducibility, and statistical programming through the use of R, R Markdown/Quarto, and Github.\n\n            \n                \n                \n                 Fall 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Advances in Statistical Methods and Research Practices in Psychology \n                \n            \n            \n                PSY505 | \n                Princeton University\n                \n            \n            Psychological methods keep evolving: psychology researchers constantly adopt new statistical methods, computational tools, and research practices. This course introduces the latest advances in research methods in psychological science through a series of lectures, tutorials, and seminars. Lectures are generally given by a guest speaker who introduces methodological and statistical issues relevant to psychology research. Students engage in a group discussion with the speaker before the lecture. Tutorials provide students with the necessary skills to build reproducible and transparent research programs. For instance, students learn how to use R, RMarkdown and Git/GitHub, or how to preregister their studies. Finally, students take part in seminars, in which they discuss timely topics in psychological methods (e.g., reproducibility, transparency, pre-registration).\n\n            \n                \n                \n                 Fall 2023\n                \n                \n                \n                 Spring 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Advanced Statistical Medthods \n                \n            \n            \n                PSY504 | \n                Princeton University\n                \n            \n            As a discipline, statistics has given profound tools to the sciences. Statistics is our way of detecting patterns in the noisy universe. Traditionally, psychologists were limited to research questions that could be answered with classical statistical analyses, but without particularly intending to do so, psychologists have found themselves at the cutting age of statistical discovery in the 20th and 21st centuries. This is because we needed to adapt our tools to meet the demands of the complex nature of the phenomena we study. This course will give you the quantitative tools to ask and answer research questions that were impossible to test as recently as a decade ago. Specifically, we will cover the following topics as stand-alone modules\n\n            \n                \n                \n                 Spring 2024\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Teaching/index.html#section-1",
    "href": "Teaching/index.html#section-1",
    "title": "Teaching",
    "section": "2022‚Äì23",
    "text": "2022‚Äì23\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Fundamentals of Statistics in Psychological Science \n                \n            \n            \n                PSY503 | \n                Princeton University\n                \n            \n            This graduate-level course covers foundations of statistics in psychological research. It is required of all first-year students in the psychology Ph.D. program. The purpose of this course is to introduce students to statistics with an emphasis on modeling. We cover many of the most widely applied data analysis models in psychology. We focus on data visualization, effect size estimation and interpretation, and using statistical analysis to inform scientific research questions. We develop practical skills related to data management, reproducibility, and statistical programming through the use of R, R Markdown/Quarto, and Github.\n\n            \n                \n                \n                 Fall 2022\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Advances in Statistical Methods and Research Practices in Psychology \n                \n            \n            \n                PSY505 | \n                Princeton University\n                \n            \n            Psychological methods keep evolving: psychology researchers constantly adopt new statistical methods, computational tools, and research practices. This course introduces the latest advances in research methods in psychological science through a series of lectures, tutorials, and seminars. Lectures are generally given by a guest speaker who introduces methodological and statistical issues relevant to psychology research. Students engage in a group discussion with the speaker before the lecture. Tutorials provide students with the necessary skills to build reproducible and transparent research programs. For instance, students learn how to use R, RMarkdown and Git/GitHub, or how to preregister their studies. Finally, students take part in seminars, in which they discuss timely topics in psychological methods (e.g., reproducibility, transparency, pre-registration).\n\n            \n                \n                \n                 Fall 2022\n                \n                \n                \n                 Spring 2023\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Advanced Statistical Medthods \n                \n            \n            \n                PSY504 | \n                Princeton University\n                \n            \n            As a discipline, statistics has given profound tools to the sciences. Statistics is our way of detecting patterns in the noisy universe. Traditionally, psychologists were limited to research questions that could be answered with classical statistical analyses, but without particularly intending to do so, psychologists have found themselves at the cutting age of statistical discovery in the 20th and 21st centuries. This is because we needed to adapt our tools to meet the demands of the complex nature of the phenomena we study. This course will give you the quantitative tools to ask and answer research questions that were impossible to test as recently as a decade ago. Specifically, we will cover the following topics as stand-alone modules\n\n            \n                \n                \n                 Spring 2023\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Teaching/index.html#section-2",
    "href": "Teaching/index.html#section-2",
    "title": "Teaching",
    "section": "2021‚Äì22",
    "text": "2021‚Äì22\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Research Methods in Cognitive Science \n                \n            \n            \n                01:185:320:01 | \n                Rutgers University\n                \n            \n            The aim for this course is to provide students with the necessary foundation to think critically about research in cognitive science. The course reviews the scientific method and considers the strengths and weaknesses ofa range of approaches, such as laboratory experimentation, neuroscience methods, and online data collection.Students will be introduced to statistical reasoning in science, including a basic overview of common statistical techniques. We will also discuss principles for the ethical conduct of research both in the laboratory and online.This course will include traditional lectures, in-class activities, and special presentations by cognitive scientists at Rutgers and other universities in the field. Students will also get hands-on experience programming and collecting their own data, which will culminate in a paper and presentation. This course counts for 3 credits.\n\n            \n                \n                \n                 Fall 2021\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Teaching/index.html#earlier",
    "href": "Teaching/index.html#earlier",
    "title": "Teaching",
    "section": "2020‚ÄìEarlier",
    "text": "2020‚ÄìEarlier\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Brain and Behavior \n                \n            \n            \n                 | \n                Iowa State University\n                \n                | Instructor\n                \n            \n            \n\n            \n                \n                 \n                 Summer 2014\n                \n                \n                 \n                 Summer 2015\n                \n                \n                 \n                 Summer 2016\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Academic Skills Learning \n                \n            \n            \n                 | \n                Iowa State University\n                \n                | Instructor\n                \n            \n            \n\n            \n                \n                 \n                 Fall 2015\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Psychology of Language \n                \n            \n            \n                 | \n                Iowa State University\n                \n                | Instructor\n                \n            \n            \n\n            \n                \n                 \n                 Spring 2016\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Introduction to Statistics \n                \n            \n            \n                 | \n                University of Alabama-Birmingham\n                \n                | Instructor\n                \n            \n            \n\n            \n                \n                 \n                 Fall 2018\n                \n                \n            \n        \n    \n    \n\n\nNo matching items"
  }
]